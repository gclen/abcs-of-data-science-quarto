[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "ABCs of data science is intended for anyone who wants to learn more about data science, regardless of skill level. It aims to give readers a high level overview of various data science concepts, so that they can explore these topics further.\nABCs of data science was created by Graham Clendenning who has worked as a data scientist in the public sector since 2015."
  },
  {
    "objectID": "posts/2020-07-01-h-is-for-hdbscan/index.html",
    "href": "posts/2020-07-01-h-is-for-hdbscan/index.html",
    "title": "H is for HDBSCAN",
    "section": "",
    "text": "There are many data science problems where you don’t have labelled data and need to use clustering to find related points. For these clustering problems, HDBSCAN is a great algorithm. It was originally created by Campello et al. and there is a fast Python implementation written by Leland Mcinnes and John Healy. When I refer to HDBSCAN I’ll be talking about the python implementation/package. It is generally the first clustering method I try for a variety of reasons:\n\nYou don’t need to specify the number of clusters. Other clustering methods such as k-means require that you specify the number of clusters to find in your data, and this is hard to know ahead of time. HDBSCAN will find the natural number of clusters in your data. All you need to specify is the minimum number of points that a cluster should have (which is much easier to have an intuition for).\nMany other clustering algorithms make assumptions about the shape of the clusters (e.g. they must fit in a circle) or they are all the same density. In real data this is generally not true and HDBSCAN finds clusters with varying shapes/densities.\nHDBSCAN will label points as noise/outliers. Many clustering algorithms force every point into a cluster. However, real world data is messy and having outliers improves the quality of the clusters (since they aren’t polluted by noise).\nIt generally just works. I find I spend much less time fiddling with parameters and spend more time looking at my actual data.\n\nMuch of this blog is based on examples in the wonderful documentation for HDBSCAN. In particular, if you want to see how HDBSCAN compares to other clustering algorithms read this page.\n\nLet’s look at an example\nThe first thing we need is an embedding, which as you might recall is just a numeric representation of your data with a way to measure distance between points. Shown below is a plot of a sample dataset. While it is an artificial dataset, it has many properties of real data:\n\nThere are a lot of noisy/outlying points which don’t belong in any cluster\nThe groups of points are different shapes and you can see in some clusters that the points are much closer together, while in others they are less dense.\n\n\nLet’s try clustering this data. First we import HDBSCAN and load our data\nimport hdbscan\nimport numpy as np\n\ndata = np.load('clusterable_data.npy')\nClustering the data is as simple as\nclusterer = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean')\nclusterer.fit_predict(data)\nHere we are saying that there must be at least 15 points close together before we say that something is a cluster. How do we measure “close together”? We also specified a Euclidean distance metric. This is the default metric but HDBSCAN can use many other metrics. Euclidean is the default metric, but it is always better to explicitly state your distance measure for other people reading the code. If we wanted to use cosine distance instead of Euclidean (despite Euclidean being the better choice in this case) we could do\nclusterer = hdbscan.HDBSCAN(min_cluster_size=15, metric='cosine')\nclusterer.fit_predict(data)\nWe can see which cluster each point belongs to using\nlabels = clusterer.labels_\nprint(labels)\n# [ 5  5  5 ... -1 -1  5]\nThe first points shown are part of cluster 5. Points that are outliers are given a label of “-1” so they are easy to filter out. Let’s remake the plot above but colour the points based on their cluster label. The outlying points (part of the -1 cluster) will be grey.\n\nYou can see that the resulting clusters are pretty good. More importantly, they match what we would intuitively pick as the clusters if we had to draw lines around the groups of points.\n\n\nSummary\nHDBSCAN and its python implementation is a fast clustering algorithm that is easy to use. It naturally handles a lot of the messiness of real world data and lets you spend more time focussing on the problem you are trying to solve. If you want to learn more about how HDBSCAN works and see other examples check out the resources below.\n\n\nOther resources\n\nBlog on understanding HDBSCAN which is similar to this blog but goes into much more detail\nHow HDBSCAN works from the official documentation\nHDBSCAN, Fast Density Based Clustering, the How and the Why - John Healy"
  },
  {
    "objectID": "posts/2020-05-30-g-is-for-gradient-descent/index.html",
    "href": "posts/2020-05-30-g-is-for-gradient-descent/index.html",
    "title": "G is for Gradient Descent",
    "section": "",
    "text": "As I’ve said many times before AI/machine learning/deep learning is not magic. In the case of supervised learning models (including [deep learning]((../2020-04-08-d-is-for-deep-learning/index.qmd) you have four things:\n\nLabelled data\nFeatures (e.g. image pixels or text)\nA weight for each feature (since some features are more important than others)\nAn objective (or cost) function which measures how well/poorly your predictions match the labels.\n\nBefore we start training a model, we have our data set and choose the features we want to use, as well as an objective function. I’ve mentioned some common objective functions in previous blog posts including cross-entropy loss and root mean squared error (RMSE). An objective function is a function of both the features as well as the weights. Once we’ve chosen the features and objective function, they are fixed while we actually train the model. This means the only thing we can change is the weight of each feature. When we refer to training a model, what we typically mean is finding which values of weights minimize or maximize the objective function. How do we actually find this set of weights? You can imagine trying a bunch of different sets of weights and seeing which gives the best model performance. However, as you might expect there are better ways to find the best set of weights. The broad category of algorithms that find the minimum/maximum values of functions are called optimization methods.\n\nGradient descent\nGradient descent (and related variants) is a popular optimization technique and it is widely used in a variety of applications, including basically all deep learning models. It is an iterative method, which means it keeps repeating the same steps until some criteria is reached. This stopping criteria is also referred to as convergence criteria. This stopping condition could be “stop when the value of the loss function doesn’t change from step to step”. Let’s imagine we have a simple loss function like the one shown below and our starting position is shown in red. This starting position is typically random, since our weights are randomly initialized. We want to figure out how to get to the bottom of this bowl shaped curve, since this is the set of parameters where our loss function is at the smallest value. As we iterate through this process, we are “learning” better sets of parameters (or weights).\n\nImagine that you are on a hill at this red dot, and need to get to the bottom of the valley. However, you are also blindfolded (which sounds like the world’s worst escape room) so you can’t see where the valley is. You also want to get to the bottom of the hill as quickly as possible, so you want to take as few steps as possible. How would you do this? You would probably try to find the direction where the hill is the steepest and take a step in that direction. You would keep repeating this process until you got to the bottom of the hill. This is exactly what gradient descent is doing. We can calculate the direction with the steepest slope by calculating the derivative (or gradient) of the loss function. We then take a step in that direction, then keep repeating the process until we reach some stopping criteria.\nThe key parameter in gradient descent is called the step size or learning rate which says how far to step in the direction of the gradient. It is really important to choose this value correctly. If we choose an appropriate value (like the one shown below) we can take a reasonable number of steps (in this case 7) to get to the bottom of the hill.\n\nHowever, if we make the learning rate too small, then we need to take a lot of steps and this means that it takes much longer to get to the bottom of the hill.\n\n\n\nHere the learning rate is too small\n\n\nIf we make it larger, then we can take fewer steps. But this also runs the risk of overshooting the minimum and even risks not converging at all.\n\n\n\nHere the learning rate is too large\n\n\nOne way around this problem of having to choose the best learning rate is called “learning rate annealing”. Basically, it means that we start with a large learning rate (so we can quickly take large steps in the right direction). As we continue, we start taking smaller and smaller steps, so that we don’t overshoot the minimum. This has a nice balance between the large and small step sizes which makes choosing an initial learning rate less tricky.\n\n\nLocal vs global minima\nUp until now, we have been talking about a very simple loss function which only has one minimum. In practice, loss functions are very messy and have many hills and valleys. Each valley has a local minima and there is one true global minimum which is actually the lowest point. In the loss function shown below, there are two minima. If we start at the red dot and use the method described above, we will get stuck in the valley on the right (a local minimum). However, we would like to get to the bottom of the valley on the left.\n\nOne technique for doing this is called gradient descent with restarts. In this technique, you periodically make your learning rate very large (and then slowly make it smaller using annealing). The benefit of this technique is that the large learning rates will help you escape the local minima, and hopefully find the global minimum (or at least a good minimum).\n\n\nGradient descent in practice\nIn practice, there are other modifications people apply to gradient descent in order to make it faster/easier to compute.\n\nThe form of gradient descent described above is known as “batch gradient descent”. This means that the entire dataset is used to compute the gradients. For large datasets this is impractical since the entire dataset needs to fit in memory. In many applications, especially deep learning, stochastic gradient descent (SGD) is used. Instead of using the entire dataset to calculate the gradient, random subsets are used (called mini-batches). While this is slightly less accurate, it is much faster and more efficient.\nMomentum is a popular addition to SGD since it makes it faster to compute, and typically gives more accurate results. Here you update the weights using the gradient, but you also use a weighted average of the previous gradients. In our example about finding your way down the hill, you would not stop and try to figure out the exact best slope down the hill for each step. You would continue in roughly the same direction and make minor direction changes as needed. Momentum does a similar thing by including the history of gradients in order to speed things up.\n\n\n\nSummary\nGradient descent is a technique used in a wide variety of applications. In particular, it is the workhorse of deep learning, and is what is used when a model “learns” weights. It is a fairly simple idea at its core, and hopefully this gave you an intuition for how it works, as well as some techniques that are used in practice.\n\n\nOther resources\n\nAn overview of gradient descent with links to more resources\nNeural Networks Demystified Part 3: Gradient Descent. This is a really great short video.\nHow do we ‘train’ neural networks ?\nStochastic Gradient Descent with momentum"
  },
  {
    "objectID": "posts/2020-12-28-r-is-for-reproducibility/index.html",
    "href": "posts/2020-12-28-r-is-for-reproducibility/index.html",
    "title": "R is for Reproducibility",
    "section": "",
    "text": "In data science (and all fields of science), being able to reproduce existing results is critical. One could argue being able to reproduce results is fundamentally what makes it science. However, many fields (including data science) are going through a “reproducibility crisis” where scientists are unable to recreate the results from their own or other experiments. There are many factors contributing to this such as\n\nDo you have the same materials (either physical or data) as the original experiment?\nCan you recreate the experimental environment?\nAre the methods listed detailed enough?\n\nEveryone agrees that reproducibility is important but that doesn’t make it less of a challenge in practice. In this blog I’ll focus on reproducibility in data science but this also applies to other fields (particularly computational) of science as well.\n\nWhy is it so hard?\nIt’s worth distinguishing between reproducibility and repeatability. Reproducibility is having another person (this can also be your future self) being able to fully recreate your results (either using the same or different methods). Repeatability (also known as replicability) is “given the same data and tools can you get the same result?”. In data science, people (including me!) tend to use the term reproducibility when they technically mean repeatability. For the rest of the blog I’ll use the two terms interchangeably.\nOne of the reasons reproducibility is hard is that things change all the time. Data changes, the tools change, there could be randomness baked into an algorithm etc. There are also other systemic factors which I will touch on later. For now, let’s just focus on the technical aspects.\n\n\nDo you have the same data?\nImagine you’re reading a paper about a model that can predict if a tweet is positive or negative. You want to reproduce their results so that you can apply the model to a different data set. You look at the methods section and read “the model was trained on a random sample of 100 000 tweets”. That’s great, but which 100 000 tweets?\nThere are many challenges associated having reproducible datasets\n\nThe datasets need to be hosted somewhere (ideally somewhere you can interact with programatically). This could be something like an Amazon S3 bucket or Kaggle. This can be expensive with large datasets.\nThe dataset should be static (or you should at least know how it’s changed). If you are reproducing a model trained on IMDB movie reviews but there are now 25000 more reviews in the dataset, this could affect the results.\nIt’s not enough to have the same raw data, the processing pipeline should be the same. For example, how did the original authors handle missing data?\nThe dataset might be sensitive! For example you might have a model trained on healthcare data where it is hard to share the underlying data.\n\nUnfortunately there isn’t a solution to this problem that’s been widely adopted. There are a couple data version control solutions (e.g. https://dvc.org/) but many organizations create their own infrastructure (if they do anything at all). Many cloud providers provide dataset versioning but this can be expensive (especially for large datasets).\n\n\nDo you have the same computational environment?\n\n\n\nTaken from http://phdcomics.com/comics/archive.php?comicid=1689\n\n\nFortunately, in data science it is easier to be able to replicate the experimental environment. In theory you are able to rerun the same code that other researchers used. This assumes a few things:\n\nThe other researchers made their code available using a version control system like github or gitlab. It’s far from guaranteed that the associated code for a paper is made available for reasons such as intellectual property or competitive advantage. There is a great website which lists popular papers and their implementations so you know before you read the paper that you will be able to see the code.\nYou are using the same operating system as the original researchers. Let’s imagine you have a Windows machine and they were running Linux. This might not seem like a big difference but it can cause discrepancies.\nYou have the same version of all the code dependencies. What if the original researchers were using Tensorflow 1.15 and you have version 2.x installed? This could cause issues if the code behaviour has changed in some way.\nYou have access to similar infrastructure. If the original model was trained on 1000 GPUs and you have a laptop, it will be impossible to reproduce the results.\n\nOne way to standardize parts of the computational environment is to use a containerization solution like Docker. This allows you to have the same code, dependencies and runtime environment. This does not solve the infrastructure issue but it provides a decent solution. Unfortunately, using docker is also complicated so might have a high barrier to entry for a lot of scientists. Spinning up the required resources using a cloud provider is also possible, but again this costs money.\n\n\nDo you have the same model parameters?\nIf you are able to reproduce the compute environment and data processing, you still might need to retrain the model. One way around this is for researchers to share pretrained models but this isn’t always done. If you need to retrain the model from scratch you might not get exactly the same results. It depends on if the algorithms are deterministic or stochastic. Deterministic means that given the same inputs, parameters, and initial conditions you will get the same output. Stochastic processes have randomness inherent in them so you will get different output for the same input if you run the algorithm multiple times. There are many places where randomness can pop up\n\nStochastic gradient descent is commonly used for optimization (e.g. in most deep learning models)\nIn many embedding and dimension reduction algorithms there is randomness in the output. Points that are close in the higher dimensional space will still be close to points in the lower dimensional space, but the position of the points themselves may change\nMany models initialize their weights randomly\n\nIt’s possible to seed random number generators so that you get the same results when generating random numbers. However, these may be buried deep in libraries or not configurable.\n\n\nOther barriers\nThere are other obstacles which prevent or discourage people from making truly reproducible models (or reproducing other peoples models). One of the biggest barriers is that reproducing others’ work is expensive both in terms of time and money. It can also be very frustrating when something doesn’t work as expected In academia researchers must publish papers which are novel in some way. It is very hard to publish a paper which says “we were able to reproduce this other work”. Researchers are incentivized to research new things rather than validating and exploring prior work. Similarly, in industry people are motivated to work on new products rather than replicating prior work.\nEven if you are motivated, there are the challenges described above. Did the previous researchers make their data, compute environment, and methods available? One way to incentivize researchers to do this is to make it a mandatory component of publishing a paper.\n\n\nSummary\nAs you might have learned, reproducibility is very important but it’s also challenging to do in practice. Fortunately it’s not all bad news. There are tools which are making it easier to do reproducible science for those who are willing to put in the work.\n\nBinder makes it really easy to run someone else’s Jupyter notebook. A Jupyter notebook is an interactive notebook which allows you to have code, documentation, and images (e.g. plots) all in the same document. They’re great for exploratory data analysis and documentation.\nMany cloud providers such as Azure provide machine learning capabilities. This makes it easier to keep track of models, parameters, and datasets.\nEasydata is a python library and git template to make it easier to do reproducible data science.\n\n\n\nOther resources\n\nAlan Turing Institute podcast on reproducible data science\nUp your bus number tutorial\nData Science’s Reproducibility Crisis\nLinear digressions episode on data lineage"
  },
  {
    "objectID": "posts/2021-02-05-x-is-for-xgboost/index.html",
    "href": "posts/2021-02-05-x-is-for-xgboost/index.html",
    "title": "X is for XGBoost",
    "section": "",
    "text": "XGBoost is a software package/algorithm that has been used to train impressive models in recent years (particularly on tabular/structured data). It’s also extremely fast and has wrappers in a wide variety of languages (Python, C++, Ruby, R, etc). For these reasons, as well as the fact that it has been used to win many Kaggle competitions it has become very popular in the past few years. XGBoost stands for “Extreme Gradient Boosting”, which despite sounding like something on the side of a pre-workout supplement, is a very useful technique for improving ensembles. In this blog I’ll explain how XGBoost and related methods work and the types of problems that it’s applicable for.\n\nEnsembling\nIf you remember from “S is for Supervised Learning” an ensemble is a collection of weak learners (such as decision trees). The idea is that each learner on its own is not particularly strong, but in aggregate they will become a stronger learner. There are two main methods to combine the predictions of the weak learners: averaging and boosting.\n\nAveraging methods\nIn a random forest model, you might have 500 decision trees which are all trained on different subsets of the data as well as different features. Each tree gets a vote based on their prediction and the class with the majority of votes is chosen as the prediction (this is analogous to the wisdom of the crowds). Let’s imagine you are trying to predict if something is a Robot or Not. If 70% of the trees predict “robot”, and 30% of the trees predict “not a robot” then the overall prediction will be “robot”. This an example of an averaging ensemble because we trained 500 trees independently and then averaged the predictions.\n\n\nBoosting methods\nIn boosting we train the individual learners sequentially instead of independently. In theory, the 2nd decision tree can learn from the mistakes of the first, the 3rd tree from the 2nd and so on. At the very least we hope it will make different mistakes. AdaBoost (Adaptive Boosting) is a popular method for boosting and is relatively straightforward to understand.\nLet’s imagine we are trying to predict which players should be drafted by the NBA from draft eligible players. We have three features: the players height, average number of points scored per game in their last two seasons, and their teams winning percentage. AdaBoost typically uses decision stumps (as compared to trees) as its weak learner. A decision stump is just a very shallow tree (with only one node and two leaves). This means each stump only looks at one variable (e.g. height) at a time. The decision stump could learn that if a player is taller than 6’6” then they should be drafted (and not drafted if they are shorter than that). Here are the steps of how AdaBoost works:\n\nTrain a weak classifier (e.g. our decision stump using height as a variable) on all of the data. Each data point is given a weight (initially they are all equal).\nCreate a decision stump for all variables (height, PPG, average wins). After training check how well each stump performs (did it classify the points correctly?).\nFor all of the data points which were not classified correctly, increase their weight. For correctly classified samples, decrease their weight. This forces the overall model to focus on points that are hard to classify.\nRepeat steps 2 and 3 until you reach the maximum number of iterations (or all points have been correctly classified).\n\nAt the end, all of the weak learners get a weighted vote to produce the final prediction.\n\n\nGradient boosted trees\nIf we want to train an ensemble of decision trees how should we do it? We can generalize boosting on decision trees to use arbitrary loss functions. Once we’ve picked a loss function we can use gradient descent to optimize it. Using this technique is typically called Gradient Boosted Decision Trees or Gradient Boosted Trees.\n\n\n\nWhen should I use ensembles of decision trees?\nModels like Random Forests or Gradient Boosted Trees are a good starting point for many problems.\n\nThey can be used for both classification and regression\nThey work on a wide variety of datasets. In particular, they work well on tabular data (think Excel spreadsheets)\nRandom forests handle missing values and categorical data well\nThey typically handle high dimensional data (large numbers of features) well\nYou can typically interpret the predictions made by these models\n\n\n\nSummary\nXGBoost is a library which implements gradient boosting and is extremely fast. It’s available in a wide variety of languages and frameworks. For Python you can install it using\npip install xgboost\nEven if you don’t use XGBoost, I recommend starting with decision tree based models as a starting point for most ML problems. You can often get further with some other models but they typically require more hyperparameter optimization and deep understanding of your data.\n\n\nOther resources\n\nEpisode of Linear Digressions on Ensembles\nScikit-learn documentation on ensemble methods\nXGBoost documentation\nIntroduction to boosted trees from XGBoosts documentation\nA Gentle Introduction to XGBoost for Applied Machine Learning"
  },
  {
    "objectID": "posts/2020-04-08-d-is-for-deep-learning/index.html",
    "href": "posts/2020-04-08-d-is-for-deep-learning/index.html",
    "title": "D is for Deep Learning",
    "section": "",
    "text": "Deep learning is a class of machine learning algorithms that has exploded in popularity in the past decade. It powers many applications including computer vision, voice assistants, and translating text. While many of these applications might seem like magic, deep learning itself is not magic, and the underlying concepts are fairly easy to understand. The goal of this is to give you a high level overview so I’ll be glossing over some of the nitty-gritty details.\nPeople train and apply deep learning models across a wide variety of problem domains and tasks. One of the most common use cases for deep learning models is classification where you are trying to predict which group of things a piece of data belongs to (e.g. classifying images as a cat or dog). In order to train this model you need to have a label for each image saying that it contains a cat or dog. A machine learning problem where you have labelled data is known as a supervised learning problem. If you don’t have labels then you need to use unsupervised techniques.\n\nDistinguishing between dogs and traffic cones\nTo make this more concrete let’s use an (admittedly ridiculous) example. Let’s say we have a bunch of pictures, and we want to classify whether the image is of a dog or a traffic cone. This seems pretty simple on the surface because a traffic cone looks nothing like a dog.\n\n\n\nHow could you even confuse the two? Adapted from https://commons.wikimedia.org/wiki/File:Golden_Retriever_puppy_standing.jpg and https://commons.wikimedia.org/wiki/File:Trafficcone.JPG\n\n\nYou can probably think of some features that would distinguish these two groups such as:\n\nIs it orange?\nIs it cone shaped?\nDoes it have fur?\n\nThis seems pretty clear cut to me. However, if you search a little bit, you can find some examples where the lines start to blur a little bit.\n\n\n\nAdapted from https://www.pinterest.com/pin/432416001701357917/ and https://commons.wikimedia.org/wiki/File:Golden_retriever_with_Elizabethan_Cone.jpeg\n\n\nIn one picture, there is orange and in the other there is a cone shape. In both cases however, they are both clearly dogs since they have fur. This means that some features are more/less important than others in being able to distinguish between the two groups (that is they have different weights).\nSo if we want to be able to effectively predict if an image is of a dog or a traffic cone, we need two things:\n\nFeatures\nA way to weight those features\n\nIt is challenging to come up with features and encode them into our program. For example, how would you code “this image has fur in it”? Even if we can code our features, it’s hard to be sure that they are useful in distinguishing between our groups of images. The appeal of deep learning is that it learns both the features and the weights. Typically, there are a large number of features and weights (way more than you would want to come up with manually), which is why things like GPUs are used to train deep learning models. We’ll go through this in more detail in the next section.\n\n\nHow does deep learning actually work?\nDeep learning typically refers to large artificial neural network models. A neural network consists of multiple “neurons”. Each circle in the image below is a neuron. The neurons are grouped into layers, classified as input, hidden, or output layers. The “deep” part of deep learning means that there are many hidden layers in the model.\n\n\n\nAdapted from https://commons.wikimedia.org/wiki/File:Artificial_neural_network.svg\n\n\nAt a very high level, the input layer derives features from the data that you pass into it. The arrows connecting the layers are referred to as weights. The first hidden layer then receives the features from the input layer, multiplied by the weights between the two layers. If there are multiple hidden layers, then the first layer acts as an input layer for the second layer and so on. Finally, it goes to the output layer where the neural network makes predictions about the input data (e.g. is it a dog or a traffic cone). There is one neuron in the output layer for each class we are trying to predict. We can compare these predictions to their actual labels (e.g. the model predicted the image was a traffic cone but it was actually a dog). Each prediction also has a probability associated with it (e.g. “I am 99% sure this is a dog”). How you actually compare the predicted and actual labels is known as the objective function (sometimes referred to as a cost or loss function). We can use this objective function to update the weights in our model (using something called backpropagation). We repeat this process over and over again until we are satisfied with the model performance. This process is known as “training” a model.\nThe exact specifics of what makes up the layers is referred to as the network architecture. People generally choose a class of architecture based on the type of problem they are dealing with. For example, if you have image data you will typically choose a class of NNs called convolutional neural networks (CNNs). Within that general group of CNNs, people typically use a predefined architecture such as ResNet. If you have text data, you would typically use a class of models called recurrent neural networks (RNNs) or a subcategory of RNNs called long-short term memory networks (LSTMs).\nLet’s go back to our dogs versus traffic cones example. If we wanted to train a model to distinguish between those two classes of images, we need to choose a couple of things\n\nA model architecture\nAn objective function\n\nSince we are dealing with images we will choose some flavour of CNN for our architecture. For the objective function we will choose something called “cross entropy loss”. Cross entropy loss not only measures if the prediction was correct or not but includes how confident the model was. In the case of cross entropy loss a lower number is better. It gives a high value if you are confident and wrong but penalizes you less if you are wrong but less sure about your prediction. Conversely, if you are confident and correct then the value of the loss function will be low. For example, if the model was very confident (e.g. 99% confident) that the picture was a dog but if it was a traffic cone then the value of the loss would be high.\nOnce we choose an architecture and an objective function, we can train the model. The weights in the model are typically initialized at random. You can also use weights from a pre-trained model (this is known as transfer learning. Training uses the labels and objective function to learn/update the weights in order to improve the predictions. I’ll talk about how the weights are actually updated (using something called backpropagation/gradient descent) in G is for Gradient Descent.\nYou might be wondering “what if there is a picture which contains a dog and a traffic cone?” This is referred to as multi-label classification where you are trying to predict all labels associated with some given input data (e.g. an image). This works in the same way, just with a different objective function.\n\n\n\nTaken from https://www.flickr.com/photos/rsmith11235/8756198755\n\n\n\n\nDoing this in practice\nIf you want to learn more about deep learning and have some experience with python, I recommend taking fast.ai’s practical deep learning for coders course. It uses a “top-down” approach in which you learn to build and train models first, learning about each component as you need to.\nIf you actually want to implement a deep learning model, you should use an existing framework. Some popular python frameworks are:\n\nTensorflow\nPytorch\nKeras\nFastai\n\n\n\nOther resources\n\nDeep learning specialization on Coursera\nDeep learning from the foundations (part 2 of the fastai course)\nNeural Networks and Deep Learning: Crash Course AI #3"
  },
  {
    "objectID": "posts/2021-02-05-y-is-for-you-should-talk-to-your-clients/index.html",
    "href": "posts/2021-02-05-y-is-for-you-should-talk-to-your-clients/index.html",
    "title": "Y is for You Should Talk to Your Clients",
    "section": "",
    "text": "Many of the previous blogs in this series have focused on the more technical aspects of data science like training models and measuring performance. In all real world data science projects, you’ll need to do more than just the technical aspects. Unless you’re doing it to learn (a perfectly valid reason!), we typically aren’t doing data science for the sake of doing data science. We don’t want to spend tons of time and effort on a data science project, just to have it dropped on the floor or ignored. It’s crucial to communicate with everyone involved in your data science projects to increase the chances of your projects being adopted/recognized.\n\nData science is a team sport\nData science involves a wide range of skills and expertise, especially in large projects. You need people who can [munge data]../2020-10-10-m-is-for-munging-data/index.qmd, train models, and have subject matter expertise in the specific problem domain. For large projects, you’ll want people with software engineering experience to help you build maintainable code and systems. The traditional Venn diagram for describing data science comes from Drew Conway.\n\nIt’s pretty unlikely that one person is an expert in all of these fields (which is why they are referred to as unicorns!). Successful data science projects usually need input from a variety of people. A more accurate Venn diagram might look like\n\n\n\nTaken from https://blogs.gartner.com/christi-eubanks/three-lessons-crossfit-taught-data-science/\n\n\nClearly there is a lot to get right for a successful data science project. In my experience, people tend to focus on the more technical aspects and ignore the customer/client angle. There are a lot more resources for the data preparation and model training parts of this Venn diagram. It’s also important to understand the marketing angle.\n\n\nMarketing your work\nIt’s important to communicate with the people who are going to actually use the models that you’ve built. People won’t just use your models for no reason, they actually need to be useful. Here are a few things to keep in mind when trying to make sure the project you’re working on is useful. You should talk to your clients (either internal or external) to figure out the problems that they need solving.\n\nUnderstand their workflow\nTalk to experts in the problem domain that you’re working in. These could be doctors, business analysts at your company, or just some trusted friends/family. Try to understand what the pain points are in their workflow and figure out ways that you might be able to help solve it. Machine learning works best on specific, well-defined problems. Talking to experts can help you pinpoint these specific problems. Saying I’m going to train a machine learning model to look at MRI images for a specific disease is a more achievable goal than saying “AI/ML for medicine”. You should make sure that everyone is on the same page and using the same language. Additionally, trust the expertise of the people you are talking to! You don’t want to be this person\n\n\n\nManage expectations\nIt’s possible you’ve been tasked by your management to “use machine learning”. You should also know by now that machine learning/AI is not magic. For people who are less familiar with the field, it can be hard to tell what is doable or still an active area of research. It’s really important to be honest about what is realistically achievable. You should also make it clear if the tools you are developing are intended to take automated actions, or help humans make better decisions. Fit into their workflow If possible you should try to avoid building shiny new tools to display your model results. People typically have workflows and existing tools that they are comfortable with. You should try to fit your model results into these tools (or at the very least link your tool from the existing ones).\n\n\nCommunicate early and often\nAs the project progresses, you should update your clients with any results that you’ve found. You should also ask for their input regularly. For example, if you hear them saying “why did the model make THAT prediction?” you might want to focus on more interpretable models. It can be very helpful to have a small group of trusted clients who are closely involved with the process. Management or software engineering teams might be familiar with Agile software development as a way to manage projects. There are some good parts that can be adapted for data science projects. Linear Digressions did a good two part series on Agile for Data Science, talking about the good and adjustments that should be made. Be clear in what you are asking for feedback on. If you are looking for feedback on the way you are displaying results (as compared to the results themselves), make that obvious!\n\n\n\nSummary\nData science involves a lot of skills and no one can do it all. We tend to focus on the technical parts of the data science pipeline, but communication and marketing are crucial skills. A person with domain expertise and the ability to communicate with end users and other data scientists is as valuable as the people training the models and preparing the data.\n\n\nOther resources\n\nPractical Tips for Real-World Data Science\nAgile for Data Science\nData scientists: beware of simple metrics\nHow to define your machine learning problem"
  },
  {
    "objectID": "posts/2021-02-06-z-is-for-zero-to-done/index.html",
    "href": "posts/2021-02-06-z-is-for-zero-to-done/index.html",
    "title": "Z is for Zero to Done",
    "section": "",
    "text": "We’ve covered a lot of topics in these 26 blogs. Kudos to you if you’ve made it this far! In this blog I want to cover what the end-to-end data science process looks like. A lot of this will depend on the goals of the project and your organization:\n\nAre you trying a smaller data science to learn about a particular topic?\nAre you trying to build a product to be used by other people?\nHow mature is your organization with regards to data science? If you’re the first one doing this, things will take longer.\nDo you have a team of people or are you doing this individually?\n\nThere are 5 major steps you need to do in most data science projects\n\nDefine your problem\nPrepare your data\nTrain your model\nGet feedback\nProductionize your pipeline\n\n\nDefine your problem\nAs you’re starting you’ll want to answer the following questions. I find it’s helpful to write down your answers or talk about them with a friend/colleague.\n\nWhat are you trying to solve? Talk about the problem that you are trying to solve. You should make this as specific as possible. If you are trying to do “machine learning for cyber security” figure out what aspect is most interesting/useful. A more specific problem is “can I predict if a file is malware or not?”\nWhy are you trying to solve it? Describe how solving this problem will benefit people. This could be benefiting yourself (a learning experience), internal clients (make them more productive), or benefit customers/the world more broadly.\nHow is this solution going to be used? Write down if the problem is solved how your solution will be used. Is it going to make decisions automatically? Is it going to give context to a human so that they have extra information? Is it trying to predict something about an individual data point or is it trying to group related things together?\nWho is asking for a solution to this problem? Document who is asking for this to be solved. These people could be domain experts, your boss, or someone else. Make sure that everyone has the same expectations. For more about this read Y is for You Should Talk to Your Clients.\nHow would you do it manually? Try to go through the process of solving the problem manually multiple times. For example, if you’re trying to figure out if a file is malware or not, sit down with some cybersecurity analysts and triage the files together. This will give you a better understanding of the domain and may give you ideas for relevant features.\nWhat tools are you going to use? If this is just an individual project you’re free to use whatever you like! If you’re working in a team you’ll need to figure out which tools people have the most experience with. In some cases you might be forced to use an existing set of tools if they are the ones adopted by your organization.\n\n\n\nPrepare your data\nWithout data, it’s pretty hard to do data science! Make sure you have data that you can use (or at least process into something useful).\n\nWhat data do you have available?\nAre there external datasets you can use to enrich your data?\nWhat data processing do you need? In all real-world cases you’ll need to do [data munging]../2020-10-10-m-is-for-munging-data/index.qmd. For text data you’ll need to apply natural language processing techniques to create features from it.\nDo you need to embed your data or do dimension reduction? In many cases you’ll need to embed your data into a feature space. For more information on that I recommend reading E is for Embeddings and U is for UMAP.\nDo you have labels? If you’re working on a supervised learning problem then you need labelled data. For more information about ways to get that read L is for Labelling Data.\nHave you explored your data? You’ll need to look at your data and see if there are missing values, visualize it, look for relevant features etc.\n\n\n\nTrain your model\nOnce you’ve processed your data you’ll actually need to train your model(s).\n\nDoes your problem require a supervised or unsupervised model? If you are trying to predict something about individual data points you’ll want to use supervised learning. If you’re trying to group stuff together you’ll want to do clustering.\nWhat kind of model should you choose? This depends on the problem and data type but here are my starting points. If you have images you might want to use deep learning (typically convolutional neural networks are used). For text or tabular data it’s a good idea to start with Random Forest or Gradient Boosted Trees. It’s worth trying to get a simple model working (even if it’s not the new shiny thing).\nAre there pretrained models you can start from? Transfer learning is a powerful technique that can let you make progress quickly.\nDo you need to do any post-processing? Especially in the case of clustering you might need to apply additional filters on your clusters for them to be useful. For example, your model could find 1000 groups of stuff but a human might only have time to look at 50 groups. Heuristics on top of your clusters can be a useful way to rank your clusters in terms of usefulness.\n\n\n\nGet feedback\nYou should continuously ask for feedback on all parts of your pipeline. This will help make sure that you and your clients are on the same page.\n\nIs the output of your models useful? If you show the results of your model to subject matter experts do they agree with it? You may want to look into model interpretability methods to try and figure out why your model made a given prediction.\nDoes your output fit into your customers workflow?\nAre your models biased in any way? This can be a hard problem to diagnose but it’s an extremely important one. For more information read B is for Bias.\n\n\n\nProductionize your pipeline\nOnce you have a successful prototype, it’s time to put it into production. As your doing so, you’ll want to think about the following:\n\nHow can you access the results of your model? One option is setting up a REST API using a framework like FastAPI. Many cloud providers like Azure and AWS offer SaaS solutions for this. Are you results available as part of an app?\nAre you retraining the model on a regular basis? You should retrain the model periodically to ensure that it’s behaving as you expect. You’ll need to measure model drift which is how the performance changes over time on new data.\nIs your workflow repeatable? For more on this read R is for Reproducibility. How easy is it for someone else to maintain your workflow?\n\n\n\nSummary\nThere are lots of moving parts involved in completing a fully productionized data science pipeline. It’s okay if it seems overwhelming! No one is an expert in everything, which makes it even more important to work with other people. If you have any questions don’t hesitate to leave a comment, send me a message on twitter send me an email at abcsofdatascience [at] gmail [dot] com.\n\n\nOther resources\n\nPractical Tips for Real-World Data Science\nApplied Machine Learning Process\nHow to Define Your Machine Learning Problem"
  },
  {
    "objectID": "posts/2020-05-20-f-is-for-f1-score/index.html",
    "href": "posts/2020-05-20-f-is-for-f1-score/index.html",
    "title": "F is for F1 score",
    "section": "",
    "text": "When we are training supervised learning models, we want to measure how well the model is performing. Choosing the correct metric for measuring model performance depends on what kind of task you are doing. There are two main categories of supervised learning tasks\n\nClassification: Here you are trying to predict which category (or categories) a piece of input data belongs to. For example, given an image you might try to predict if it is a picture of a dog or cat.\nRegression: Here you are trying to predict a numerical label. For example, you might try to predict the selling price of a house given some features about it, such as neighbourhood, number of bedrooms etc.\n\nIn this blog I’ll cover a couple of different methods for measuring model performance. First we’ll focus on classification tasks. To make this more concrete let’s imagine we are training a model to predict if a cookie contains either chocolate chips or raisins.\n\n\n\nTaken from https://commons.wikimedia.org/wiki/File:Chocolate_chip_cookies.jpg and https://commons.wikimedia.org/wiki/File:Raisin_cookie.jpg\n\n\n\nThe problem with accuracy\nOne way of measuring model performance is called classification accuracy (more commonly referred to as accuracy). This is simply “how many predictions did you get correct out of how many predictions did you make?”. If you correctly predicted the type of cookie 95 times out of 100 predictions, your accuracy would be 95%. However, accuracy only works well if the number of items in each category is roughly equal. If there are many more items in some categories than others, we call this class imbalance. For example, chocolate chip cookies are much more popular than raisin cookies in general. If we assume out of 100 random cookies that 99 of them are chocolate chip, we could get 99% accuracy by guessing chocolate chip every single time. In practice this would be a pretty terrible model, but according to accuracy this is a good model.\n\n\nBuilding a confusion matrix\nIf we want to try to address this problem we can try building a confusion matrix (which is less scary/confusing than it sounds). A confusion matrix shows all of the possible combinations of predictions vs the actual labels. We need to pick one category as the “positive class” and the other as the “negative class”. This is arbitrary, so let’s pick chocolate chips as the positive class, and raisin as the negative class. There is a sample confusion matrix shown below.\n\n\n\n\n\n\n\n\n\nPredicted: Chocolate chip\nPredicted: Raisin\n\n\n\n\nActual: Chocolate chip\n120 (True Positives)\n2 (False Negatives)\n\n\nActual: Raisin\n10 (False Positives)\n23 (True Negatives)\n\n\n\nI’m sure we’ve all had the experience of picking up what we thought was a chocolate chip cookie. Upon biting into it we realize “ugh, raisin”. Don’t get me wrong, I like raisin cookies but it is the mismatch between expectation and reality that is the problem. This mismatch is referred to as a false positive (FP) since we predicted the positive class, but it was actually the negative class. As you would expect, there are also false negatives (FN) where you predict raisin, but it’s actually chocolate chip. If the prediction matches the actual label, these are referred to as true positives (TP) or true negatives (TN).\n\n\nPrecision and recall\nPrecision and recall are ways of measuring classification quality.\n\nPrecision\n\\[ \\text{Precision} = \\frac{TP}{TP + FP}\\]\nThis is the number of true positives, divided by all of the positive results predicted by the model. In our cookie example, this means “when you predicted chocolate chip, how likely was it to actually be chocolate chip?” If we look at the confusion matrix above, the precision would be\n\\[ \\text{Precision} = \\frac{120}{120 + 10} = 0.923\\]\n\n\nRecall\n\\[ \\text{Recall}= \\frac{TP}{TP + FN}\\]\nThis is the number of true positives, divided by all of the points that should have been classified as positive. More concretely, this is “out of all of the chocolate chip cookies, how many did you find?”\n\\[ \\text{Recall} = \\frac{120}{120 + 2} = 0.984\\]\n\n\n\nF1 score\nA model with high precision but low recall, returns few results but the predictions generally correspond to the actual labels. On the other side, a model with high recall but low precision returns many results, but most of the predictions are incorrect when compared to the labelled data. Obviously, we would like a model with both high precision and high recall. A metric called F1 score combines both precision and recall, and it is a common way to measure model performance\n\\[F1 = 2\\frac{P \\cdot R}{P+R}\\]\nAn F1 score of 1.0 corresponds to perfect precision and recall and is close to zero for an extremely bad model. The F1 score is just one way of combining precision and recall, and there are other F measures which weight precision/recall differently.\n\n\nThings to consider\nIn some problems, false positives are more important than false negatives. In others, the opposite is true. Imagine we are trying to predict if a patient has a certain disease or not. A false positive means we think they have the disease, but in actuality they are healthy. Depending on the side effects of treatment, a wrong prediction could have severe consequences. If there are major side effects to treating the disease, we may want to favour precision over recall. On the other hand, there may be cases where treating the disease has minor side effects, and leaving the disease untreated has major consequences. In this case we would want to favour recall, where we find as many instances of the disease as possible. This is obviously a complicated subject, and I highly recommend listening to this episode of the podcast linear digressions if you want to know more.\n\n\nRegression\nNow we’ll briefly talk about measuring the performance of regression models. Let’s imagine we have a model which predicts the selling price of a house based on it’s square footage. Here we don’t have a small number of categories (e.g. a house could sell for \\(&lt;/span&gt;503 200 or &lt;span&gt;\\)632 777). Below I’ve plotted some fake data where the black points show the predicted price, and the blue points show the actual selling price. One way of measuring the amount of error is called Mean Absolute Error (MAE). This is simply adding up all the differences between the predicted and actual values (shown by the red lines) and dividing by the number of points. The absolute part of MAE just means that you take the absolute value of the differences. If you have one prediction which overestimates the price by \\(&lt;/span&gt;5000 and another which underestimates by &lt;span&gt;\\)5000, the MAE is 10000 (not 0 where the two differences cancel out). A related metric is called Mean Squared Error (MSE) where you square the differences before adding them up. The reason for this that big differences will become even bigger. As an example, a difference of \\(&lt;/span&gt;2 will become 4 but a difference of &lt;span&gt;\\)1000 will become 1000000.\n\n\n\nSummary\nWhen working on a supervised learning problem, choosing the correct metric is important. First you should think about if you are working on a classification or regression problem. Then you need to consider which metric best measures what you are trying to achieve. This is just a small summary of some of the ways of measuring model performance. For more info check out the links below or look at the description of cross-entropy loss in [“D is for deep learning”]((../2020-04-08-d-is-for-deep-learning/index.qmd).\n\n\nOther resources\n\nOther metrics you can use\nMore on precision and recall\nBeware of simple metrics podcast"
  },
  {
    "objectID": "posts/2021-01-17-s-is-for-supervised-learning/index.html",
    "href": "posts/2021-01-17-s-is-for-supervised-learning/index.html",
    "title": "S is for Supervised Learning",
    "section": "",
    "text": "Supervised learning is one of the major categories of machine learning, and it helps us predict something about a given piece of data. Many of the concepts about supervised learning have come up in previous posts, but I’m hoping that this will provide a clearer picture of how they all fit together.\nSupervised learning means that you have a labeled training set with some set of features and the given output. One category of supervised learning is regression which happens when you have continuous output data. For example, given features like the square footage of a house, predict the selling price. Classification is the other major category of supervised learning which occurs when you have discrete output (i.e a finite number of categories). For example, you are trying to predict if an email is spam or not. There are only two possible outcomes in this case (spam or not spam). In regression there are an infinite number of possibilities (i.e. the house could sell for \\(&lt;/span&gt;500 000, &lt;span&gt;\\)500 001, $500 002 etc).\n\nHow does supervised learning work?\nTo train any supervised learning model you need to have four things:\n\nLabelled data\nFeatures\nAn objective (or cost) function\nA weight for each feature\n\n\nLabelled data\nAs we’ve discussed before, getting high quality labelled data is hard. It’s often extremely time intensive (and also very boring). Depending on the problem it can also require a substantial amount of expertise. For example, anyone can label images as having a dog or a cat but it requires medical professionals to label MRI scans for a particular disease.\n\n\nFeatures\nThis is the processed data that gets put into your model. [Processing your data]../2020-10-10-m-is-for-munging-data/index.qmd and extracting relevant features is typically the bulk of the work involved in training a ML model. In images the raw pixels could be used as features (this is common in deep learning). Natural language processing techniques are common for extracting features from text data. Getting useful features from tabular (i.e. in a table or CSV) data usually involves some level of expertise about the problem itself. In many cases you can get much better performance from a model by improving the features, as compared to using more sophisticated models.\n\n\nObjective functions\nAn objective function (also called a cost function or loss function) tells us how well our predictions match the labeled data. It should give us more information than if the prediction was correct/incorrect. If the prediction was wrong, it should also tell us how wrong the prediction was. An example of an objective function for regression problems is Mean Absolute Error (MAE). This is just the difference between the actual value and predicted value. When trying to predict house prices being off by \\(&lt;/span&gt;300 000 is worse than being off by &lt;span&gt;\\)1000. For classification problems, one common objective function is cross-entropy loss. It takes into account if the prediction was correct as well as how confident the model was about the prediction. If you are confident about a correct prediction you will be rewarded, but if you are confident about an incorrect prediction you’ll be penalized heavily. However, if you are not confident about your prediction the reward/penalty will be much lower. There are more examples of objective functions in [D is for Deep Learning]../2020-04-08-d-is-for-deep-learning/index.qmd and F is for F1 Score.\n\n\nFeature weighting\nLet’s imagine we have an extremely simple model. We are going to try to predict the price of a house given two features: location (the distance to a major city), and if the house has a dishwasher or not. As you might expect, some features (e.g. location) are more important than others (e.g dishwasher). When we start training a model, the input data, features and objective function stay the same. The only thing that changes is the weighting of each feature. Typically these weights are chosen randomly before training. In our house example on the first iteration we might have\n\\[ \\text{Prediction} = (0.33)\\cdot(\\text{Location}) + (0.67)\\cdot(\\text{Has dishwasher}) \\]\nHere the dishwasher feature is twice as important as the location. This would probably lead to a bad model. After a few training iterations we might have\n\\[ \\text{Prediction} = (0.99)\\cdot(\\text{Location}) + (0.01)\\cdot(\\text{Has dishwasher}) \\]\nThis makes more intuitive sense. Obviously the location matters a lot more but all other things being equal, a house with a dishwasher would sell for a bit more.\n\n\n\nTraining a model\nTraining a model refers to finding the feature weights which minimize/maximize the objective function. For some objective functions we want to find the minimum value, while for others we want to find the maximum value. There are different ways of finding these min/max values (called optimization methods) but gradient descent is an extremely common one.\nWhen we train a model we want to ensure our model does not overfit/underfit the data. We can do this by splitting our dataset into a training and test set. We only update the model weights based on data in the training set. The test set is used to evaluate the model on data it hasn’t seen. For more details on preventing over/underfitting see K is for K-fold Cross-Validation.\nThere are multiple factors that go into choosing an appropriate model for your problem. These include things like performance, training speed, interpretability, and data types. That being said three of the most common model types are:\n\nLogistic regression\nRandom forests\nDeep learning models\n\n\n\nBuilding a cookie classifier\n\nLogistic regression\nLet’s imagine we have a training set where cookies are deemed acceptable or unacceptable. Here we have two features: the cookie area and the chocolate chip density. In the plot below the blue circles indicate acceptable cookies while the red squares are rejected (though I’d probably still eat them).\n\nUsing this training set, we can train a model to predict which of some new set of cookies (the test set) will be acceptable. One way that we could use to do this is called logistic regression which is a fairly popular algorithm. In logistic regression we are trying to learn the decision boundary which is shown above by the dotted black line. Points inside this circle are classified as acceptable while points outside are rejected. This decision boundary isn’t always a circle (in fact it usually isn’t). One nice feature of logistic regression is that you can have an arbitrarily shaped decision boundary (careful not to overfit!). As you can see, the classification isn't perfect and there are red squares misclassified as acceptable and vice versa. We can measure how well our classifier is doing using its F1 score.\nClassification algorithms typically have some level of confidence about their prediction (via a probability). So it may have classified the blue circle on the edge of the decision boundary as unacceptable but if you looked at the confidence it would probably be pretty low. The same thing is true for the misclassified red squares. This is particularly useful if you are doing something like classifying executable code as malicious or benign. You can order your predictions in terms of confidence to have the most malicious at the top and then get less confident as you scroll downwards.\n\n\nRandom forests\nThere are two key concepts involved in a random forest model that are more broadly used in data science/machine learning as a whole. The first is the idea of a decision tree. A decision tree essentially looks like a flow chart. As you go down the tree, the data is categorized further and further. The figure below shows an example decision tree of our cookie classification example. As you can see it is much closer to what a human would think if they were trying to determine whether the cookie should be sold or not.\n\nThe second key idea is the notion of ensembles. In an ensemble you take a bunch of weak learners (like a decision tree) and aggregate them together to create a strong learner. People also do ensembles with different machine learning algorithms (e.g. an ensemble of random forest, logistic regression, and support vector machine models) where each model has a vote in the final classification. There are different ways to create an ensemble (e.g. votes can be weighted) and they are an extremely useful tool.\nThe forest portion of random forest comes from the fact that it is an ensemble of decision trees. But where does the random come from? The random forest algorithm works as follows:\n\nSample data your training set (with replacement) to create random subsets of data\nFor each subset choose a random set of predictor features (e.g. cookie size)\nFor each node in the tree, find the predictor variable that provides the best split of the data. For example you might find that cookies under 5cm are not worth selling. At the next node choose another set of features and repeat the process.\n\nAs a typical example you might have 100 decision trees (all trained on random sets of features) in your random forest. Each tree makes a prediction if the cookie is worth selling/not. The final prediction is made based on the class that gets the most votes.\n\n\n\nSummary\nSupervised learning is an extremely powerful tool that can help us automate many tasks and scale to levels where humans cannot. A common use case for supervised learning models is to automate the simple decisions and have humans look at the more complex cases. As always it’s important to have interpretable models as well as an appeals process for wrong predictions. While model choices are important, data preprocessing and feature selection can have a large effect on model performance. It’s usually better to start with a simpler model like logistic regression or random forests before moving into more sophisticated models like deep learning.\n\n\nOther resources\n\nAndrew Ng’s ML course\nPractical machine learning for coders\nCrash Course AI #2: Supervised learning"
  },
  {
    "objectID": "posts/2020-05-03-e-is-for-embeddings/index.html",
    "href": "posts/2020-05-03-e-is-for-embeddings/index.html",
    "title": "E is for Embeddings",
    "section": "",
    "text": "In supervised learning, we have labels which tells us the group(s) a data point belongs to. However, in unsupervised learning (where we don’t have labels) we need to calculate how similar points are from one another. To do this we need to have an embedding which consists of two things:\n\nA numeric representation of your data (because we need to do math)\nA distance measure (so we can determine how close/far two points are from one another)\n\nEach embedding gives you a different way to look at your data, depending on the features/numeric representation and distance measure you choose.\n\nLet’s solve a mystery\nTo give you a better idea about what embeddings are, we’re going to look at people hanging out in the Clue mansion. However, instead of solving a murder mystery, we are going to try to determine groups of friends of people in the mansion.\n\nIf I asked you to guess which people (shown as X’s) were most likely to be friends in the image above, you’d probably say the people in the ballroom or lounge. This is because they are physically close together. But how can we say they are “close together”? Implicitly in your head you did this using an embedding. You probably looked at the position of people on the board (i.e their x,y coordinates) so we have a numeric representation of the data. We also know that the people in the ballroom are closer to each other than they are to the person standing outside the billiards room. How do we know this? Imagine drawing a line between the person closest to the piano and the person directly to their left. Now imagine drawing another line between the person closest to the piano, and the person just outside the billiards room. The length of the first line is shorter than the second line, so we say that the two people in the ballroom are closer to each other. This method of measuring distance is called Euclidean distance, and it’s probably what you think of first when you need to determine how close/far something is. However, there are other ways to measure distance.\n\n\nDistance measures\nThere are many kinds of distance measures (or metrics) but here are a few popular/useful ones. I’ll also describe some additional distance metrics in a later blog.\n\nEuclidean distance\nThis is just the length of the line drawn directly between two points as we discussed above.\n\n\nManhattan/city block distance\nLook at the two people outside the billiards room. If you were playing the game and wanted to move one to the other, you could not move them on a straight line between them. You’d have to move 1 over and then 3 up/down for a total of 4 moves. This is similar to Manhattan (also known as city block) distance, which is the sum of the distances in each direction. In the illustration below, the Manhattan distance between the two points is 12. If you were walking you would need to walk 12 city blocks (assuming you can’t walk through buildings), but there are multiple routes you could take. Manhattan distance is useful for higher dimensional data where Euclidean distance breaks down in what is known as “the curse of dimensionality”.\n\n\n\nTwo points with a Manhattan distance of 12. Taken from https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Manhattan_distance.svg/1200px-Manhattan_distance.svg.png\n\n\n\n\nCosine distance/similarity\nAnother way we can measure distance is by looking at the cosine of the angles between vectors. To make this more concrete, let’s imagine we have some data on how often people spend time in particular rooms in the mansion.\n\n\n\n\nBilliards room\nStudy\nLibrary\n\n\n\n\nMr. Green\n3\n0\n0\n\n\nMs. Scarlet\n1\n2\n2\n\n\nProfessor Plum\n0\n4\n4\n\n\n\nIn this case, Mr. Green has been in the billiards room 3 times but has never been in the study or library. Looking at these counts we would probably say that Ms. Scarlet and Professor Plum are more similar to each other than they are to Mr. Green. We can think of these counts as vectors in a coordinate system, where instead of an X or a Y axis we have a “Billiards room” or “Study” axis. Below I’ve plotted the vectors below and you can see that the angle between the Ms. Scarlet and Professor Plum vectors is smaller even though the lengths of the vectors are quite different.\n\nCosine distance lets us say that two things are similar if they have similar sets of “stuff” even if the frequencies (i.e. vector lengths) are different. If two vectors are pointing in the exact same direction, the angle between them is 0 degrees (even if the length of the vectors are different). The cosine of 0 is 1, so we say they have a cosine similarity of 1. To convert this to a distance, we do 1-similarity since we want the distance between perfectly similar things to be 0. If two things are completely dissimilar, the angle between them is 90 degrees and the cosine similarity is 0. One benefit of cosine similarity is that it takes the direction of the vectors into account, where two vectors pointing in the exact opposite direction will have a similarity of -1.\n\n\n\nChanging feature sets\nLooking at the physical distance between people in the house is one way to try and determine friend groups. There are other ways that you could measure this. For example you could give everyone a survey about their hobbies/interests. The participants put an X if they are interested in a particular hobby, and leave it blank if they aren’t. This would give you a matrix where a 1 corresponds to a X and a 0 corresponds to a blank:\n\n\n\n\nBoard games\nBaseball\nDancing\nCooking\n\n\n\n\nColonel Mustard\n0\n1\n0\n0\n\n\nMrs White\n1\n0\n1\n1\n\n\nMrs Peacock\n0\n0\n1\n1\n\n\n\nYou could then use cosine distance to determine which people have similar interests to one another. This potentially gives you a different view of your data, where the person in the study is (physically) very far from everyone in the first feature set, but could have interests very similar to other people in the second feature set.\n\n\nThings to keep in mind\nAll unsupervised machine learning techniques require an embedding. It’s important to think about which feature set/numeric representation and distance metrics you’re using in order to make sure they will help you answer the questions you’re trying to solve. I find it’s helpful to think about “How do I know if two things are similar? What makes them similar?”.\nMany techniques implicitly use a certain distance metric (e.g. k-means clustering uses Euclidean distance). You’ll need to make sure that you’re using a technique that allows you to specify a distance metric, or uses one that is correct for your problem. Additionally, you’ll want to make sure that your features are on the same scale (you may need to do some normalization). For example, if you have some data which includes ratings on a 1-5 scale and other ratings on a 1-100 scale, then that will affect your results. Choosing the right embedding requires thinking about your problem, as well as trying out different feature sets and distance metrics.\n\n\nOther resources\n\nEmbed all the things - John Healy (talk from Pydata Los Angeles 2019)\nDifferent Types of Distance Metrics used in Machine Learning"
  },
  {
    "objectID": "posts/2021-02-03-v-is-for-visualization/index.html",
    "href": "posts/2021-02-03-v-is-for-visualization/index.html",
    "title": "V is for Visualization",
    "section": "",
    "text": "One of the most important aspects of data science is being able to effectively communicate and use your data to tell a story. Often you’ve spent a lot of time and effort cleaning your data, training models, and exploring your data. Visualizing your data is a crucial aspect of being able to tell that story and show off any insights you might have gained. You should always keep the following questions in mind:\n\nWho is your audience? Are they other data scientists? Subject matter experts? Other stakeholders (e.g. executives)?\nWhat are you trying to convey? It’s worth thinking of visualizations as representations of one aspect of your dataset. Try to make sure you are clearly communicating one aspect/concept.\nAre your visualizations easy to understand?. You’ve probably spent the most amount of time exploring your data. For people who are less familiar, they need to be able to quickly understand what you are showing. You don’t want people to dismiss the rest of your hard work because of some boring or complicated visualizations.\n\nHow do you make an effective visualization? One of the first questions you need to ask is what type of data you have.\n\nTypes of data\nThere are two broad categories of data, quantitative and qualitative, and each have a couple sub categories.\n\nQuantitative\nThis is numeric data and represents something like a measurement or a count. There are two subtypes\n\nContinuous: Here we can have an infinite number of values (e.g. 2, 5.983, 8749). Good examples of continuous data include height, temperature, and average selling price. WIth continuous variables you can compute statistics such as mean, median, standard deviation etc.\nDiscrete: These are still numbers but can’t be meaningfully divided into smaller chunks. For example, you can have 1 or 2 children but not 1.5 children. However, you can still compute statistics such as mean or standard deviation. You could say the average household had 1.54 children.\n\n\n\nQualitative\nQualitative data is something that can be categorized based on traits/characteristics (as compared to numbers).\n\nCategorical: Categorical data is data that fit into a finite number of categories that have no inherent ordering. Colours (blue, red, green) are classified as categorical data because one colour is not inherently better than another.\nOrdinal: Ordinal data is a type of categorical data with at least three categories. As you might guess from the name there is an inherent order. A good example of ordinal data is agreement data (strongly disagree to strongly agree) commonly found in surveys.\n\nHere are some general rules of thumb for plotting different data types:\n\nIf you have categorical data use a bar chart (especially if you need precise comparisons!). For example, here is a plot comparing the most frequently used tools on the DataIsBeautiful subreddit.\n\n\n\n\nTaken from http://www.randalolson.com/2016/03/11/what-data-visualization-tools-do-rdataisbeautiful-oc-creators-use/\n\n\n\nUse a scatterplot or line plot if you’re comparing two quantitative variables. Here’s a plot comparing NBA/NCAA shooting accuracy vs distance from the basket.\n\n\n\n\nTaken from https://toddwschneider.com/posts/nba-vs-ncaa-basketball-shooting-performance/\n\n\n\nIf you have qualitative data (or multiple comparisons in quantitative data) add extra attributes to visually distinguish between data types. You should also not just rely on colour (as some people have trouble distinguishing between colours). For example, texture (making your points dotted or striped) is very effective for categorical data as it provides a visual marker to differentiate between your categories. But this is not effective for quantitative (i.e. is striped bigger than dotted?). Keep these attributes in mind the next time you are making a plot, and think about what is the best way to represent your underlying data.\nIf you need to communicate uncertainty in measurements, you will need to add error bars\n\nOf course there are many more chart types than histograms, bar charts, scatter plots, and line charts. It’s important to consider the type of data you are plotting as well as your audience. For example, a violin plot can be enlightening for other data scientists but might not be as intuitive for other people.\n\n\n\nPlotting libraries\nThere are loads of plotting libraries (as shown in the graph above) but here are a few of my favourites (mostly Python libraries):\n\nMatplotlib is extremely powerful but there can be a pretty steep learning curve. Fortunately there are many libraries that wrap matplotlib but still let you dive into it when needed. Pandas plotting is one such wrapper.\nSeaborn also wraps matplotlib and has a lot of great functionality. For example, pairplot will create a plot for every pair of variables in the dataframe you pass it.\nggplot2 is an R library which is very powerful and can make very nice looking plots.\nBokeh lets you make interactive plots in Python and is particularly handy when using Jupyter notebooks\n\n\n\nThings to consider\nOnce you’ve decided on what concept you want to convey and picked the appropriate chart type, there are a few other things to consider.\n\nCheck that your colour scheme works for both your data and the platform you’re presenting it on. For example, if you’re presenting categorical data, don’t use 7 shades of blue. Your chart might look great on your 5K display at your desk but could look washed out and hard to decipher on the projector screen when you’re giving a presentation.\nEnsure your axes make sense. Your axes don’t always need to start from zero. In fact you should zoom in if you want to show data fluctuation more precisely. Just don’t make your graph look like this\n\n\n\nBe careful about using logarithmic axes. For the right audience (who are used to interpreting them) they can be okay but you need to make it clear that it’s a nonlinear scale. You can try normalizing your points to the mean instead.\nAvoid overplotting. Overplotting is when data or labels in a visualization overlap so it can be hard to distinguish individual points. Try making your points semi-transparent or hollow circles so that it makes it easier to see specific points.\n\n\n\n\nTaken from https://python-graph-gallery.com/wp-content/uploads/134_Fighting_overplotting1.png\n\n\n\n\nSummary\nEffective visualizations can help you deliver a story about your data/models and any insights that you’ve found. In addition to convincing stakeholders/clients, they can be extremely helpful in exploratory data analysis to convince yourself about your understanding of the data. There are many things to consider (and it’s easy to make misleading charts) when creating a visualization but it’s worth getting it right.\n\n\nOther resources\n\nData Visualization 101: 7 Steps for Effective Visualizations\nilovecharts\nGuide to Data Types and How to Graph Them in Statistics\nSpurious correlations\nHow to Lie with Data Visualization"
  },
  {
    "objectID": "posts/2020-12-27-q-is-for-q-learning/index.html",
    "href": "posts/2020-12-27-q-is-for-q-learning/index.html",
    "title": "Q is for Q-learning",
    "section": "",
    "text": "How can we train a computer to play a video game? Here the goal could be something like “beat a level as quickly as possible” or “collect as many points as you can” or both of those objectives at the same time. This is a very different task than the ones we’ve seen previously, such as trying to tell if a picture contains a dog, or how to group data together. To accomplish this we can use a framework of algorithms called reinforcement learning (or RL). Reinforcement learning tries to train an agent to learn a set of actions to achieve some goal. RL is used in a wide variety of disciplines such as\n\nPhysics/chemistry. We might want to improve the yield of a particular chemical reaction by changing things like temperature or concentration. Doing this manually is time consuming and requires a lot of resources/expertise. We can train an RL model to learn experimental parameters which will help optimize the reaction.\nFinancial trading\nRobotics\n\nIn all of these disciplines you need to have a simulation of the environment you want to explore. In the case of chemistry, you need to be able to simulate the effect of increasing the temperature of a chemical reaction. Learning to play a video game is an ideal task for RL since the simulation already exists.\n\nPlaying Super Mario Bros\n\nLet’s imagine we’re trying to train a computer to learn to play Super Mario Bros. Here we have the computer (the agent) controlling how Mario can move. It has a set of actions it can perform:\n\nMove Mario to the left\nMove Mario to the right\nRun\nJump\nPause the game\n\nThe overall goal for RL algorithms is to figure out what is the best set of actions to take for a given state. For example, at the beginning of the level it’s better to move to the right instead of the left (or jumping in place). To figure out what the “best” set of actions is, we also need a specific goal such as “collect as many points as possible”. We do this by specifying something known as a reward function. In reinforcement learning the agent tries a bunch of different actions to try and maximize this reward function.\nIn the case of collecting as many points as possible, the reward function would go up the more points Mario gets. However, if our reward function is just based on the number of points, the agent will spend all it’s time trying to collect every coin, powerup etc. We also want to incentivize the agent to complete the level quickly. We can add penalties to the reward function to incentivize this behaviour. For example, we might subtract some number for each second Mario takes to complete the level. Or since Mario only has a small number of lives, we might subtract a large number from the reward function if Mario dies.\nHere is the general strategy for reinforcement learning:\n\nObserve the environment\nTake some action(s) (e.g. moving right) based on some strategy\nReceive a reward/penalty\nUpdate the strategy based on the reward/penalty\nPerform many, many iterations until you find an optimal strategy\n\nIn RL there is a tradeoff between exploration and exploitation. Exploitation means doing the actions which you know will give you a reward (e.g. going for a particular coin block and getting 100 points). However, there may be actions that give an even bigger reward (e.g. grabbing the top of the flagpole gives 5000 points). You need to have a balance between exploitation (keep doing what gives you value) and exploration (trying new actions).\n\n\nTypes of reinforcement learning\nThere are two main categories of RL:\n\nModel based: Here the model estimates or predict the optimal strategy (or policy) based on the reward function itself. For example, in a given state the algorithm can say “if I jump I know the reward function will increase”\nModel free: Here the algorithm estimates the optimal policy without direct knowledge of the reward function. They rely on samples from previous iterations (e.g. jumping under a coin block gives a reward) to learn the best strategy. Q-learning is an example of a model free RL algorithm.\n\n\n\nQ-learning\nThe “Q” in in Q-learning stands for quality. In this case quality means how useful a certain action is for gaining some future reward. Here, the agent maintains a Q-table which is a table of states and possible actions. The Q-table is used to calculate the maximum expected future reward for performing a certain action in a given state. Let’s walkthrough the Q-learning algorithm at a very high level\n\nStep 1: Q-table initialization\nInitially the Q-table is all zeroes (since we don’t know what actions will give rewards). In our Mario example, the initial Q-table could look like\n\n\n\nState/action\nMove left\nMove right\nJump\nHold run button\n\n\n\n\nFirst screen\n0\n0\n0\n0\n\n\nSecond screen\n0\n0\n0\n0\n\n\n…\n…\n…\n…\n…\n\n\nEnd of level\n0\n0\n0\n0\n\n\n\n\n\nStep 2: Choose an action\nThere are two ways of choosing an action: randomly or based on the maximum value in the Q-table. Initially, all of the actions will be random (this is exploration) as we learn which actions lead to rewards. Eventually, most of the actions will be based on the Q-table.\n\n\nStep 3: Perform the action\nHere the agent performs the action (e.g. jumping). It’s worth noting that the order of the actions matters a lot. For example, if Mario is trying to jump over a pit he must first run to the right then jump (not jump then run to the right).\n\n\nStep 4: Calculate the reward\nHere we measure the reward based on the agents actions.\n\n\nStep 5: Update the Q-table\nHere we update the Q-table based on the agents actions and the reward. This helps the agent learn the expected future reward for a series of actions.\n\n\nStep 6: Repeat\nSteps 2-5 are repeated many, many times until training is stopped or some condition is met. At the end of training we should hopefully have a strategy which maximizes the reward function. After training the agent can choose actions for a given state based on the maximum value in the Q-table. In our Mario example the agent should learn that the best strategy at the beginning of the level is to run right then jump on the goomba (instead of just choosing actions at random).\n\n\n\nSummary\nReinforcement learning is a very powerful area of machine learning, and still an active area of research. When using RL it’s critical to make sure the reward function matches the objective you are trying to achieve. If you would like to try training RL algorithms in practice, I recommend looking at the tutorials for the OpenAI gym package.\n\n\nOther resources\n\nLinear digressions episode on RL\nQ-learning example in OpenAI gym\nUsing RL to beat Mario"
  },
  {
    "objectID": "posts/2020-01-15-a-is-for-ai/index.html",
    "href": "posts/2020-01-15-a-is-for-ai/index.html",
    "title": "A is for Artificial Intelligence",
    "section": "",
    "text": "I was recently in San Francisco and throughout the city there are many, many billboards containing slogans like “Mission Critical AI” and “Enterprise AI”. There is no doubt that usage of the phrase “artificial intelligence” or AI has increased dramatically in recent years. This increased usage has made it difficult to tell what “using AI” even means.\n\nSo what is AI?\nMany people have differing opinions on how to strictly define AI. When people refer to AI this is generally synonymous with machine learning (ML). Since it’s so hard to come up with a definition, let’s define what AI is not:\n\nAI is not magic\nAI is not perfect (more on that in B is for Bias)\n\nIt’s hard to come up with a strict definition for AI since the generally agreed upon definition has evolved over time. In the 1980s people called large rule based systems (“expert systems”) AI. These systems required subject matter experts and programmers to define a set of rules. As you can imagine (or remember) this is a very time intensive process and there are lots of edge cases to consider. Nowadays people typically think of deep learning (more on that in D is for Deep Learning) as AI. This requires lots of labeled training data (which often needs to be labeled by experts) but models can learn their own rules about the data.\nIn both cases the “intelligence” part of AI is a bit of a misnomer. Another term that has been suggested is “cognitive automation”. We are trying to teach a machine to perform a set of tasks based on our knowledge of the world. This is different from something that is truly intelligent that can reason about the world and learn abstract concepts. This distinction is important and referred to as “general” versus “narrow” AI.\nGeneral AI (also referred to as strong AI) refers to a machine that is “human-like” (think HAL 9000 or Skynet). As you would expect from the name, it can generalize it’s previous knowledge to new problem domains. This means that It can intelligently perform a wide variety of tasks without needing explicit training data. Voice assistants like Alexa, and Siri seem like they can generalize, but as anyone who has used them knows, they definitely have limits. They tend not to understand context in a way that a human would, and questions may need to be rephrased in order to be interpreted correctly. A true general AI doesn’t currently exist and it will probably be a while before it does.\nNarrow AI (or weak AI) refers to a machine that is good at specific tasks (e.g. image recognition) but can’t generalize to different domains. For example, you could train a machine learning model to distinguish between cats and dogs but it would not be able to answer the question “Where is the best pizza in Ottawa?”. Narrow AI works by using predefined rules or learning from lots of (probably labelled) data.\n\n\nWhat is AI used for?\nAI/ML is widely used in a large variety of industries for a number of tasks. This includes:\n\nRecommending videos on youtube\nDetecting if a computer has malware\nFinancial trading\nHelping doctors make diagnoses\nVoice assistants like Google Assistant\n\nMany of these tasks are applications of fields such as:\n\nComputer vision (processing images and audio)\nNatural Language Processing (NLP)\nRecommender systems\n\nThe fact that AI is a buzzword means that it is applied to many different products (even if they don’t include any ML models). This over usage of the term causes many people to twitch when they hear it (and judge people who do use the term). However, it’s important to keep in mind your audience when you are talking about machine learning. Some audiences may not be familiar with specific technical terms (point them at this blog series ;) ) but have heard the term AI.\n\n\nSummary\nAI/ML is a rapidly growing field which is being applied to a large variety of domains. It’s hard to separate what is real from the snake oil. I’m hoping this blog series will give you enough background knowledge to think critically when you hear the term AI and know the limitations of AI/ML models. In the next blog I’ll talk about many of these limitations and how they affect people’s everyday lives.\n\n\nOther resources\n\nCrash Course Artificial Intelligence\nHello World - Hannah Fry"
  },
  {
    "objectID": "posts/2021-02-04-w-is-for-wasserstein-gans/index.html",
    "href": "posts/2021-02-04-w-is-for-wasserstein-gans/index.html",
    "title": "W is for Wasserstein GANs",
    "section": "",
    "text": "You may have seen or heard of the term “deepfakes” where realistic looking images/videos/audio have been generated to look or sound like a specific person (e.g. a famous politician). Deepfakes is a more colloquial term for the field of synthetic media which is the use of AI to generate images/audio/videos. Faking images is not a new phenomenon (e.g. photoshop) but historically faking video has been challenging. One of the main drivers of the recent breakthroughs in synthetic media are the use of Generative adversarial networks (GANs). While there are some concerns about how GANs and related techniques will be used more broadly, there are legitimate purposes for it. For example, Apple used GANs to generate images of faces where they knew the direction the person was looking. This makes it easier to get training data that can be used for other models.\n\n\n\nImage of a synthetically generated face. Taken from https://commons.wikimedia.org/wiki/File:AI_generated_face.jpg\n\n\n\nGenerative Adversarial Networks\nGANs are a technique created by Ian Goodfellow which involves two [neural networks]../2020-04-08-d-is-for-deep-learning/index.qmd pitted against one another. One neural network (called the generator/creator) tries to create realistic looking data (typically starting from random noise). The other network (the discriminator/investigator) is fed a combination of fake and real data and it tries to predict whether the data is real or fake. This is a zero-sum game, so any gains made by the generator are lost by the discriminator. As training goes on, both models will get better at their tasks and the generated data will be more realistic. To steal an analogy from this great episode of Linear Digressions it’s like one person is trying to counterfeit money and the other person needs to determine counterfeit from the real money. Below is a high level view of the architecture of a GAN. Initially the generated data is pretty clearly fake\n\nAfter training the generated data will become much more realistic\n\nIn a standard GAN, the discriminator outputs a probability that the input is real or fake (i.e. some number between 0 and 1). The loss function is typically based on the Jensen-Shannon divergence (JS divergence) which is a way of measuring the similarity of two probability distributions. The generator wants to minimize this loss (i.e. the real and fake distributions should look the same), while the discriminator wants to maximize it. It’s worth noting that after training you have both a model that can generate realistic data as well as a model that can distinguish between real and fake data.\n\n\nWasserstein GANs\nStandard GANs can be hard to train in practice. This is because you need to find some stable equilibrium for both the generator and discriminator. Wasserstein GANs (also known as WGANs) make a few modifications to the standard GAN which make it better in practice.\n\nInstead of using JS divergence, they use Wasserstein distance (also known as earth-mover distance) to measure the similarity of the two probability distributions. This has some nice theoretical justifications (i.e. the math works out).\nThey call the discriminator a “critic”. Instead of having to output a probability (which is restricted to a number between 0 and 1), WGANs don’t have this constraint. This means that there can be bigger differences between the losses, leading to better training.\nThe critic is updated more often than the generator (e.g. 5x more)\n\nThere are a few other implementation details but these are the bigger ones. Overall they are easier to train and more stable, producing better results. WGANs are just useful for generating images. They’re used to generate text and other kinds of data such as realistic looking domain names.\n\n\nStyleGAN\nStyleGAN is another GAN extension which was developed by Nvidia. It’s not directly related to WGANs but has produced impressive looking images of faces. I’m not going to go into how they work here, but there is a good overview written by Cody Wild.\n\n\n\nSummary\nGANs are being used in a wide variety of applications including making art, music, and generating training data for other ML models. GANs and synthetic media can definitely seem scary and there are societal implications to consider in addition to the technical ones. This is a very active field with research into generating more realistic media as well as detecting if an image is fake or not. Tools to detect synthetic media will become increasingly important but education can play a large part in making the public more critical in the images/videos they see. While generated outputs are always improving, there are subtle signs to look for to tell if an image is fake or not. This includes things like asymmetry (e.g. someone wearing one earring), noise in the background, or other oddities. Here is a good article detailing the things to look for when trying to tell if an image is fake.\n\n\nOther resources\n\nHow to Implement Wasserstein Loss for Generative Adversarial Networks\nWasserstein GAN paper\nEpisode of Linear Digressions on GANs\nEpisode of Linear Digressions on deepfakes"
  },
  {
    "objectID": "posts/2020-10-05-k-is-for-kfold-cross-validation/index.html",
    "href": "posts/2020-10-05-k-is-for-kfold-cross-validation/index.html",
    "title": "K is for K-fold cross-validation",
    "section": "",
    "text": "In supervised learning problems we have data as well as a label (something we need to predict) for each data point. We typically refer to the combination of data + labels as training data. You might remember from F is for F1 score that there are two broad categories of supervised learning problems: classification and regression. In classification, we are trying to predict which class (or category) the input data belongs to. For example, this might be trying to predict if an email is spam or not spam. There are typically a small number of categories to choose from (e.g. spam/not spam). In regression problems, we are trying to predict a continuous variable (i.e. a number) such as the selling price of a house based on some features (e.g. square footage). Here we are not trying to pick from a small group of categories, but to get close to the actual number (where there can be infinitely possible values).\nIn all supervised learning problems we want to know well our model is doing and how well it can predict things for unseen data.\n\nHow does our model fit the data?\nLets focus on regression and take a further look at the housing example. For now assume we have a training set and some way to model our data. How do we know if our model is any good? Let’s take a look at our imaginary training data\n\nAs you can imagine, as the square footage of a house increases the price goes up. If our model tries to fit this data with a straight line then we get something like this\n\nThis line is an okay fit to the data but doesn’t really give accurate predictions. We say that this model underfits the data or has high bias. What we really want is a function that is much closer to all the points\n\nThis is a pretty good fit to the data and seems to do what one would intuitively expect. The function doesn’t pass through every point but it’s pretty close to most of them. Our model could probably find some function that passes through every point\n\nWhile this function passes through every point it won’t work well on new data. We say that this model overfits the data or has high variance. This means that the model has two many parameters or there are not enough features in the training set. In this case it would probably be beneficial to add more features to the training set. For example, there is a huge difference in price between a 1000 square foot house in San Francisco versus Thunder Bay. So adding the location of the house might improve the model. In the case of the model that underfit the data, adding more features won’t help. We need to add more parameters to the model so that there is more room to try and fit the data.\n\n\nTesting our model\nSo how do we tell if our model is any good? Typically, the training data is broken into 2 parts. There is a training set (about 70% of the data) and a test set (the remaining 30%). Splitting the data into training/test sets is known as cross-validation. As the name implies the parameters of the model are trained using the training set. The model is then fed in the data from the test set and the error is calculated. Using the error associated with both the training and the test sets you can diagnose if your model underfits, overfits, or is a good fit to the data.\nIn addition to using a train/test split, people will often use a third split: the validation set (sometimes called a development set). Let’s break down what each of those sets are used for:\n\nTraining set: As you would expect, this is the data used to actually train the model. As you might recall, training a model just means learning which feature weights give you the best predictions (i.e. match your labels as closely as possible).\nValidation set: This is used for learning the best set of hyperparameters. Hyperparameters are knobs you can turn for the model itself (e.g. learning rate or mini-batch size). In the training set where we are trying to answer “how can I weight these features to get the best predictions?”. In the validation set we are trying to answer “how can I adjust the model itself to give the best predictions when training?”\nTest set: We want to test our model on data that it has not seen in order to see if it is overfitting/underfitting. We never update the model (or hyperparameters) on the test set and we just make predictions on it using the trained model.\n\n\n\nK-fold cross-validation\nIf you split your data into training/validation/test sets using a 60/20/20 split then that means you can only use 60% of your available data for model training and the validation set is a fixed 20% of the data. In cases where you don’t have a lot of training data (which is often) this extra 20% can make a big difference! K-fold cross-validation lets you get the benefits of having a validation set without having to hold out 20% of your training data.\nIn k-fold cross-validation you have a training set and what I will call the true test set. This test set is exactly what is described above: unseen data used to test predictions. The k-fold part comes from taking your training set and splitting it into k groups (or folds). The value of k can be any number that you choose but as an example let’s say k is 5. You would split your training set into 5 folds and train the model 5 times. The first time you train, the first fold is used as the “test” set and the remaining 4 folds are used as a training set. The second time you train, you use the second fold as a “test” set and the other 4 folds are used as a training set. You repeat this process until you’ve used each fold as a test set. After you have fully trained the model you test your predictions on the true test set as before. This process is illustrated in the image below (taken from the wonderful scikit-learn documentation). This process can be more computationally expensive since you need to train k models instead of 1 but also means you don’t need a held out validation set.\n\n\n\nTaken from https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\n\n\nHow do you choose a value for k? There are some different methods for choosing k since a wrong value can lead you to be overconfident in your model:\n\nPick k=10. This is a decent starting point (or k=5) and has been shown to work in a wide variety of applications.\nPick a value of k so that each fold gives a statistically representative sample size\nLet k be the number of points you have in your training set so that each point can be used in the hold out test set. This is also referred to as leave-one-out cross-validation.\n\n\n\nSummary\nEvaluating the performance of supervised learning models is critical and you want to be sure that your models aren’t overfitting/underfitting the training data. Cross-validation is a technique that should be used in all supervised learning applications to check that your model is behaving as expected. It is also important to keep in mind that even if your model generalizes well to the test set, it may not generalize to the real world if your training data is not truly representative.\n\n\nOther resources\n\nCross-validation explanation in scikit-learn\nK-fold cross-validation explanation"
  },
  {
    "objectID": "posts/2020-03-01-c-is-for-clustering/index.html",
    "href": "posts/2020-03-01-c-is-for-clustering/index.html",
    "title": "C is for Clustering",
    "section": "",
    "text": "Clustering is useful when you want to find groups of related items. For example, these items could be documents, malware samples, or customers. Clustering is also referred to as unsupervised learning since it does not involve labelled data. High quality labelled data is often hard to get, so clustering is often a good method to analyze your data. We often use clustering methods (along with visualization) when doing exploratory data analysis (EDA) since it allows us to understand how the data clumps together.\n\nWhat else can you use clustering for?\n\nAnomaly/outlier detection\nSince you know which group an object belongs to, you can say “show me the items which don’t belong to a group”. You can also look for points on the outer edges of clusters since they may also be anomalous.\n\n\nLabelling points\nYou can even use clustering to help you label your data! Instead of labelling 100 images as dogs/cats individually, you could have groups of images which are much easier to label (i.e. “here are 24 dogs all grouped together”).\n\n\n\nIs this just an excuse to post cute animal pictures? Maybe.\n\n\n\n\n\nHow do you actually cluster points together?\nTake a look at the image below (courtesy of Leland Mcinnes). You can probably see which points should be grouped together but some points clearly look like they don’t belong to any groups.\n\n\n\nTaken from https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html\n\n\nThe simple definition of clustering “find similar groups of stuff” sounds fairly simple at the surface. If you think about it a bit more you’ll realize that there are a number of questions that need to be answered:\n\nHow do you define similarity? In every clustering algorithm you need to specify a way of measuring the distance between two points. This is often referred to as a distance metric. As you would expect, points are similar if they are close together (the distance between them is small) and less similar the farther apart they are. There are many kinds of distance metrics, and choosing the right wone depends on your data. For more about distance metrics see “E is for Embeddings” and “J is for Jaccard Metric”.\nDoes every point need to belong to a group? Can a point belong to multiple groups? The answers to these questions depend on the algorithm. In some cases, points can only belong to one group. This is known as hard clustering. In other cases they have a probability associated with being part of a cluster. This is known as soft clustering (or fuzzy clustering). Some clustering algorithms will force every point into a group (whether it belongs in there or not), while others will just label those points as outliers/noise.\n\n\n\nTypes of clustering methods\nSince the definition of cluster is so broad, there are many types of clustering algorithms. Here are a few broad categories:\n\nCentroid based\nIn these algorithms, clusters are determined based on which centroid you are closest to. A centroid is just a point calculated by averaging all of the other points in a group. One popular centroid based method is k-means which works as follows.\n\nPick k centroids (where k is the number of clusters in your data). For the sake of argument, let’s say we have 2 centroids.\nRandomly place the centroids\nCalculate the distance from each point to the centroid, and pick the closest centroid as its cluster id. For example if a point is closest to centroid 1 then it is part of cluster 1\nRecalculate the centroids as the average of all of the points in the cluster. Centroid 1 is now the average of all points in cluster 1, centroid 2 the average of all points in cluster 2, etc.\nRepeat steps 3 and 4 until your clusters stabilize (i.e. the cluster ids for your points stop changing)\n\n\n\n\nTaken from https://stanford.edu/~cpiech/cs221/img/kmeansViz.png\n\n\nThere are a few issues with this method. The first is that it is really hard to know how many clusters you have ahead of time (i.e it’s hard to choose k). The second is that it doesn’t allow for clusters of different shapes since it assumes that all clusters are spherical. Finally, every point is forced into a cluster.\n\n\nDensity based\nIn density based clustering algorithms, clusters are areas with lots of points. If you look at the image above, this probably closely aligns with your intuition about what is/is not in a cluster. The clusters you identified have high density (lots of points in a small area). DBSCAN (density-based spatial clustering of applications with noise) is a popular density based clustering algorithm. It has two main parameters, min_points and epsilon. In order for something to be labelled a cluster, there must be at least min_points close together. If min_points is 10, that means there will be no clusters with less than 10 data points. Epsilon is basically “how far from a point do I look to see if it is close”. For each point, you can see how many other points are within epsilon. If there are at least min_points, then you have a cluster. In the image below, all of the red points (A) have at least 4 points (min_points=4) within epsilon distance. The yellow points (B and C) are also in the cluster because they are reachable from one of the red points. N would be labelled an outlier since it is not within epsilon of any points.\n\n\n\nTaken from https://en.wikipedia.org/wiki/File:DBSCAN-Illustration.svg\n\n\nChoosing min_points is much easier than choosing the number of clusters ahead of time (which we needed to do in k-means). However, picking the correct value for epsilon can be tricky and is more of an art than a science. Another issue with DBSCAN is that it assumes all of the clusters have the same density, which is not always the case.\n\n\nDistribution based\nIn these algorithms, you train sets of statistical models and points are assigned to a cluster based on which model they are most likely to occur in. One benefit of this type of algorithm is each point has a probability associated with belonging (e.g. 0.75 probability of being in cluster A, and a 0.25 probability of being in cluster B). This means soft clustering is baked into the algorithm, and if you want hard clusters (i.e. a point can only belong to one cluster) you can just choose the largest likelihood. One popular algorithm in this class is Gaussian Mixture Models, where you start with a set of randomly initialized Gaussian models and then update their parameters based on your data. Learning these parameters is done using an interative method called expectation-maximization algorithm. One downside of these types of models is that they may overfit your data. That is the model will fit your data very well but not generalize well to new data.\n\n\nConnectivity based\nConnectivity based clustering is also known as hierarchical clustering. One of the core ideas is that things can be grouped together at different levels. For example a picture of a golden retriever could be grouped with other retriever breeds, all dogs, common pets, all animals, etc. In this case other pictures of retrievers would be close, other dogs slightly further away, and then other animals further away from that. This creates a dendrogram (tree) and the clusters are determined by where you cut in the tree. For example in the figure if you cut at the first branch, you would have two clusters: one containing lemurs, and the other containing all of the pictures of cats/dogs. Cutting at the next branch down you would have three clusters: lemurs, cats, and all dogs.\n\nThere are basically two ways to create this dendrogram. The first is a bottom up approach where every point starts as its own cluster and then you merge clusters together. In the second method everything starts as its own cluster and then you iteratively break everything apart until each point is it’s own cluster. The difference between algorithms in this class basically differ on how you choose to measure the distance between clusters.\n\n\n\nHow do I choose which clustering (or class of) algorithm to use?\nSome of this depends on your data. In practice many algorithms combine elements of these classes. One example of this is HDBSCAN (hierarchical DBSCAN), which is an extremely useful clustering algorithm. I’ll talk about HDBSCAN in more detail in H is for HDBSCAN but HDBSCAN is a good first choice. If you’re looking for a comparison of different clustering algorithms on the sample data I showed above, take a look at the link “Comparing clustering algorithms” in the resources below.\n\n\nOther resources\n\nComparing clustering algorithms\nClustering: a guide for the perplexed - John Healy and Leland Mcinnes\nCrash course AI: unsupervised learning"
  },
  {
    "objectID": "posts/2020-10-18-n-is-for-natural-language-processing/index.html",
    "href": "posts/2020-10-18-n-is-for-natural-language-processing/index.html",
    "title": "N is for Natural Language Processing (NLP)",
    "section": "",
    "text": "Natural Language Processing (NLP) is a huge area within data science. It’s so huge that this blog will barely scratch the surface and will just give you a flavour of the kinds of things people try to use NLP for. As you might guess, the goal of NLP is to try and gain insights and information from language (either spoken or text). Text data can come from a wide variety of sources such as tweets, news articles, or transcripts of speech-to-text. NLP is used in a lot of applications, including\n\nAutocorrect\nChatbots and virtual assistants (e.g. Siri or Alexa)\nLanguage translation (e.g. Google translate)\nDocument summarization\nText classification (e.g. is this email spam or not?)\nSentiment analysis (e.g. is this movie review positive or negative?)\nGrouping documents\n\nIn recent years, machine learning (deep learning has become increasingly popular within NLP but there are still a number of non-ML based techniques.\n\nLanguage models\nIf you’ve ever tried to learn a new language, you probably know that languages are hard. There are lots of weird rules (i.e. grammar) and there are even more exceptions to those rules. Language also changes depending on the context. For example, the language used in academic papers is very different from tweets. So given a corpus (the technical term for a bunch of data such as documents) we want to learn how language is used within that set of documents. A document could be a tweet, an academic paper, an email etc. It’s nearly impossible to code all of the grammatical rules ahead of time, so we try to use NLP techniques to model language as it’s used in that corpus. The goal of this is not to relearn grammar, but give a better footing for the task that we really care about (e.g. sentiment analysis).\nA common practice in many NLP tasks is to use a language model which lets us learn how specific words are used in a corpus. For example, words such as “the” or “and” occur much more frequently than say “lagniappe”. To train a language model, we take a bunch of text and then try to predict the next word. If we have the sentence “I have a golden retriever and she is the best” we want to use the previous words to predict the next word.\n\nGiven “I”, predict “have”\nGiven [“I”, “have”], predict “a”\nGiven [“I”, “have”, “a”], predict “golden”\nContinue until you’ve predicted the number of words in the sentence\n\nWe repeat this process and compare how our predictions match the actual text to improve the model. At the end of this we will have a predictive model for how different words are used in practice (e.g. “the” is much more likely than “pizza”). This is obviously a challenging task (and the model will often be wrong). Fortunately we can train models on huge amounts of text (e.g. wikipedia). We don’t even need extra labels since we already know what the next word is in a given sentence! In practice we can use language models that have already been trained so we don’t need to train a new language model on wikipedia for every task. Language models are typically used as the starting point for other downstream tasks such as text classification. In some cases they are used directly in applications like predictive text/autocorrect on your phone. The benefit and downside of language models is that they model how a language is used. This means that if enough people type something incorrectly it’s possible that the model will start suggesting the incorrect version. How a language model performs (which will then affect downstream task performance) is typically dependent on the amount of preprocessing done (more on that later).\n\n\nFinding spam emails\nLet’s imagine we want to train a model to predict if an email is spam or not spam (ham). This is an example of a text classification problem.\nRegular email:\n\nHey,\nWant to grab lunch today? There’s a new taco truck downtown that looks great :)\n-Alice\n\nSpam email:\n\nDear valued customer,\nYour invoice is attached. In order to see your purchase history click here\nSincerely, A totally legitimate business\n\nFirst we need to turn the corpus of emails into a format that our machine learning model can understand (i.e. numbers). This is called vectorization. The simplest thing we could do is to count how often each word appears in each document. Unsurprisingly, this is called count vectorization. This gives us a word-document matrix where each row corresponds to a document and each column corresponds to a word. The values in the matrix are how often each word occurred in a given document. As an example, let’s say we have the following three short emails (documents):\n\nThe boss wants the report by Friday.\nPizza half price! This Friday only!\nI ordered the pizza for the party.\n\nOur word-document matrix would look something like this (for brevity not all words are included)\n\n\n\n\nthe\npizza\nreport\n…\nfriday\n\n\n\n\nDoc 1\n2\n0\n1\n…\n1\n\n\nDoc 2\n0\n1\n0\n…\n1\n\n\nDoc 3\n2\n1\n0\n…\n0\n\n\n\nThe columns are known as the vocabulary since it is the unique set of words occurring in all documents. As you might imagine this matrix could get very big if there is a big vocabulary (and lots of documents). However, the matrix will be sparse (mostly filled with zeroes) since most words will not appear in most documents. Fortunately, computer scientists have lots of ways to deal with sparse matrices so this is not a problem in practice.\nYou might notice that the word columns aren’t in the same order as the words in the original documents. We call this a bag-of-words model since we throw out all word ordering. Using a bag-of-words model means that we lose some information but it’s much faster computationally and it works surprisingly well in practice. Of course there are some applications (e.g. the language models described above) where order does matter.\nCount vectorization is very simple where we just count how often a word appears in a document. But how do we figure out what the words are? What if words are slightly different (e.g. “Pizza” and “pizza”)?\n\n\nPreprocessing\nPreprocessing is a catch-all term for anything we do to text before passing it into a model (including vectorization). You can potentially drastically improve the performance of your model by using more sophisticated preprocessing techniques. That being said, it’s often worth trying the simple things first!\n\nTokenization\nTokenization is where we split some text into tokens (e.g. words). Taking a sentence and splitting it into words seems simple enough right? It’s easy enough to split a sentence on spaces and then use the resulting words. There are also more sophisticated tokenization techniques which will split within words (e.g. turning #datascience into “#” and “datascience”). This is also related to chunking where you try to find the sentence boundaries in large pieces of text. How you tokenize a sentence also depends on the language. For example, a language like German which has a tendency to make new words by combining a bunch of existing words. You might want to split the new word into its original components.\nRelated to tokenization is the notion of n-grams. These are sequences of tokens which have n elements. For example, if we split the sentence “the dog loves treats” into bigrams (n=2) we would have\n[(“the”, “dog”), (“dog”, “loves”), (“loves”, “treats”)]\nThis lets you capture a little more context around each word. Once we have n-grams we can just do count vectorization like we did above. Instead of columns corresponding to words (unigrams) they will correspond to n-grams. This means that our matrix is not “how often did this word appear in this document” it is “how often did this sequence of words appear in this document”.\nIt’s worth noting that you don’t need to have traditional language to tokenize. For example you could take file paths “/this/is/a/file/path” and split it into individual files/directories ([“this”, “is”, “a”, “file”, “path”]). Once you have tokens you can apply a wide range of NLP techniques.\n\n\nLowercasing all tokens\nA really common (and easy) preprocessing step is to make everything lowercase. This means that “Friday” and “friday” are not treated as two separate tokens.\n\n\nStop word removal\nStop words are words that occur very frequently in a given language/corpus. In English these are words such as “the”, “and”, “they” (though there is no definitive list of stopwords). In many cases we want to filter out stop words since they don’t carry much information. This is more useful in tasks like text classification. In other cases such as automated translation you will need to keep stop words.\n\n\nStemming/lemmatization\nStemming and lemmatization are used to help normalize text. There are many forms of words that all have the same base. For example, “the dog barks/barked/is barking” are all semantically similar. If we are training a model (say a language model) “barks”, “barked”, “barking” will all be treated as separate tokens. To make it easier we would like to normalize all of those tokens to “bark”, giving us the sentence “the dog bark”. Stemming turns a word into its base (e.g. barked to bark), using language specific rules for removing prefixes or suffixes. However, there are many edge cases so it’s not 100% effective. This is done Lemmatization is a more sophisticated form of stemming and normalizes words into their true base (e.g. normalizing “was” to “be”). Again, this is based on language specific rules (and a bunch of lookup tables).\n\n\nMinimum term/document frequency\nIf we keep every word that shows up in any of our documents our vocabulary size will be enormous. In order to reduce the vocabulary size, one common trick is to only keep words/tokens that only occur more than N times. For example, if there is a document that contains the token “maewpfaefamefaef” it’s pretty unlikely that it’s going to show up frequently. So we can just get rid of this by saying “don’t include words that occur less than 5 times”. Similarly, we can also drop words if they show up in less than N (e.g. 5) documents. For example, if we had a document that was “Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo” the word buffalo occurs more than 5 times. But if that was the only document that buffalo appeared in, we probably want to drop buffalo from our vocabulary.\n\n\n\nBeyond simple counting\nIf we just count how often words appear in documents, there are going to be words (e.g. “the”) which occur frequently but don’t contain much information. How do we deal with the fact that some words convey more information than others? One way to do this is to weight our counts using “Term Frequency - Inverse Document Frequency” (TF-IDF). The intuition behind TF-IDF is as follows:\n\nIf a word appears frequently in most documents in the corpus it probably doesn’t give much information. So we should give those words less weight since they don’t mean as much.\nIf a word appears frequently in a small number of documents then it probably has more information. For example, the word “inheritance” might appear more often than you would expect in spam emails, but not in most normal emails. We should give these words more weight.\nIf a word doesn’t occur that frequently, then it doesn’t really give useful information. For example, if the word “oxymoron” occurred 10 times in our email corpus it doesn’t really help us distinguish between spam/not spam.\n\nIn TF-IDF vectorization, we do count vectorization as we did before then apply one additional step. This extra step is just multiplying the counts by the weight of each word. Using this weighting will help our model distinguish more easily between spam and not/spam.\n\n\nSummary\nThis was just a brief introduction to some of the concepts used in NLP. There are many things that can make NLP more complicated in practice such as dealing with multiple languages in the same corpus. NLP techniques can be a really powerful toolset to have at your disposal and they don’t just apply to traditional text data. If you have data that you can tokenize, then you can apply all of the techniques described above. If you want to dive into some NLP projects I recommend starting with this course from fast.ai.\n\n\nOther resources\n\nA Code-First Introduction to Natural Language Processing\nBecause Internet: Understanding the New Rules of Language\nNatural Language Processing: Crash Course AI #7\nTokenization strategies"
  },
  {
    "objectID": "posts/2020-10-10-m-is-for-munging-data/index.html",
    "href": "posts/2020-10-10-m-is-for-munging-data/index.html",
    "title": "M is for Munging Data",
    "section": "",
    "text": "You might think that data scientists spend most of their time training machine learning models. In fact most of the time (60%-80%) spent on a data science project is acquiring and preparing the data. In the case of supervised learning problems this also includes getting labels. This process of preparing data is often referred to as data munging or data wrangling. Data wrangling typically includes a number of tasks:\n\nGetting and cleaning the data\nSelecting features\nEngineering features\nTransforming the data\n\nTo paraphrase the bodybuilder Ronnie Coleman “everybody wants to be a data scientist but don’t nobody want to munge no ugly-ass data!”. While it isn’t the most glamorous part of data science, good data preparation is critical for having models that perform well. Properly cleaned data and good features can give better performance than trying to tweak the model itself.\n\nGetting data\nDatasets can come in a wide variety of formats but there are a few common ways of accessing them:\n\nSpreadsheets: These are typically excel spreadsheets or CSV (comma separated value) files. These can be easily read in using something like the pandas library in python.\nWeb APIs: Many data sources provide an API for fetching data, and you can interact with these APIs using python’s requests module. If you need to scrape the website itself, the beautifulsoup module is extremely helpful.\nDatabases: Data scientists will often interact with their data in databases using SQL.\nUnstructured data: All of the formats above provide data in some structured manner. Unfortunately, this is not always possible and you may need to use regular expressions and other techniques to parse the data.\n\n\n\nCleaning and transforming data\nWhen you are first looking at a new dataset it is extremely important that you look at your data! A fancier term for looking at your data is Exploratory Data Analysis (EDA). Once you have an environment set up, what does EDA look like in practice?\n\nLooking for missing data\nReal world data is messy and there can be mistakes or missing data. You might need to infer missing values or drop rows with too much missing data. How you infer missing data depends on the data type and what works best for your problem. For example, if you have a field with missing numbers, you could fill in the missing value with the average or just put 0. You also need to check if there is a pattern to which rows/fields have missing values. If there is a pattern (i.e. it isn’t random) you will need to compensate for that as well.\n\n\nVisualizing data\nData visualization is an invaluable tool when exploring data. Below are some questions that we are typically trying to answer by visualizing data\n\nWhat do the feature distributions look like? This could be as simple as answering the question “Do some values occur very frequently?”. This might be answered by making a histogram of your data.\nDo some features correlate with one another? For example in a census dataset, the neighbourhood someone lives in will typically correlate with household income.\nAre there big patterns that jump out in the data? It might be hard to see these patterns (e.g. lots of duplicate points) when looking at a spreadsheet, but when plotted are very obvious.\n\n\n\nTransforming data\nIt is pretty common to have to tweak your data into a format that a machine learning algorithm expects. Let’s imagine we have a dataset about video games\n\n\n\n\n\n\n\n\n\n\nTitle\nMetacritic score (/100)\nIGN score (/10)\nGenre\n\n\n\n\n\nThe Legend of Zelda: Breath of the Wild\n97\n10\nAction-adventure\n\n\n\nUntitled Goose Game\n81\n8\nPuzzle\n\n\n\nJames Bond 007: Nightfire\n80\n7\nFirst person shooter\n\n\n\n\nHere we have two numeric columns (metacritic score and IGN score) but they are on different scales. One goes from 0-100 while the other is from 0-10. Some ML algorithms assume that all of the features are on the same scale, so we would need to normalize these features. This could mean converting all the numbers so that they are between 0 and 1 (e.g. 8/10 becomes 0.8).\nMany ML algorithms assume that all of your input is numeric. How do we convert the genre field (which is a categorical value) to a numeric one? One simple way to do this is known as one-hot encoding. This just means representing all categories as a vector where there is a 1 if the category matches and a 0 if it doesn’t. Using our example above we have:\n\n\n\n\nAction-adventure\nPuzzle\nFirst person shooter\n\n\n\n\n\nThe Legend of Zelda: Breath of the Wild\n1\n0\n0\n\n\n\nUntitled Goose Game\n0\n1\n0\n\n\n\nJames Bond 007: Nightfire\n0\n0\n1\n\n\n\n\nSo instead of having the genre value of “Puzzle” for Untitled Goose Game, we would have [0, 1, 0]. A one-hot encoding is a very simple version of an embedding.\nIf we have text data there are many techniques that we can use which I’ll talk about in the next blog N is for Natural Language Processing.\n\n\nCommon tools for EDA\nJupyter notebooks are a popular environment for doing EDA, since it provides an interactive development environment where you can see output (e.g. plots) inline with your code. The pandas module in python is very commonly used for data munging tasks and has a lot of useful utilities. I’ll talk more about the pandas library in P is for Pandas.\n\n\n\nFeature selection and engineering\nWhen preparing data for training a model you need to figure out which features in your data will be most relevant for the problem you are trying to solve (e.g. classification). There may be features that would be helpful that don’t exist in the data as it comes in. Creating new features is referred to as feature engineering. Both feature selection and engineering require some expertise in the problem domain. One way to determine useful features if you have labels is to look at the features which are strongly correlated with those labels. Imagine you are trying to predict if a file is malware or benign. If there are attributes of that file that occur frequently when the file is malware and don’t occur when it is benign, that would be a useful feature. In the case of classification, you are looking for features that make it easy to discriminate between the classes. In the case of clustering you need to choose features that say “these points are similar if they have X in common”. Feature selection/engineering is more of an art than a science and can involve some trial and error.\n\n\nSummary\nData munging is a crucial part of data science (and you could argue it’s the majority of data science). Properly cleaning and normalizing your data can have huge benefits for the downstream task you are trying to solve. It is challenging and sometimes frustrating (looking at you regular expressions!) task but is necessary to understand your data and train an effective model.\n\n\nOther resources\n\nMore on data preparation\nWhat’s really so hard about feature engineering (episode of the Linear Digressions podcast)"
  },
  {
    "objectID": "posts/2020-09-27-i-is-for-interpretability/index.html",
    "href": "posts/2020-09-27-i-is-for-interpretability/index.html",
    "title": "I is for Interpretability",
    "section": "",
    "text": "Machine learning models are increasingly used to make decisions in everyday life, and as we’ve discussed before, they can be far from perfect. In addition to being able to appeal the decisions made by a model, it is critical to be able to interpret their predictions. Doing this in practice opens up a huge number of questions including\n\nWhat does interpretability mean?\nWhy does it matter?\nHow do we actually interpret models in practice?\n\n\nPinning down interpretability\nThis paper titled The Mythos of Model Interpretability by Zachary Lipton, gives a really good overview of what people mean when they say “interpretability”. He argues that interpretability is not just one monolithic concept, but a number of distinct ideas. It’s worth looking at the different reasons we try to interpret ML models.\n\nDo we trust the model?\nUsing a ML model to automate some task often requires a human giving up control. We often care about the kinds of predictions the model gets wrong and if there is a pattern to these incorrect predictions. For example, many facial recognition models have significantly worse performance on people of color than white people. In this case using the model with this bias would be unacceptable. However, there may be other cases where the model gets things wrong in the same way humans do. In that case it may be acceptable to use the model to make predictions.\n\n\nCan we use the model to learn something about the world?\nResearchers will often look at which features of a trained model are most important in making predictions. Imagine you have a model which is trying to predict if a patient has lung cancer. As input features you have the number of years the patient has smoked cigarettes, and the number of years they have chewed bubble gum. Of course, smoking correlates much more with lung cancer than chewing bubble gum and that feature would have a much greater importance. It’s important to keep in mind that correlation does not equal causation. For example, [the per capita consumption of margarine in the US strongly correlates with the divorce rate in Maine(http://www.tylervigen.com/spurious-correlations)]. Researchers can then use these important features to create experiments to test if those correlations are causal.\n\n\nWill the ML model help a human make better predictions?\nA common use of ML models is to help a human make a more informed decision. Imagine you work in cybersecurity and are tasked with finding malware. Fortunately, you have a ML model which determines if files are malware or benign. The important features used by the model (e.g. is the code heavily obfuscated?) can help you make a better decision in your investigation. ML models can also help you find similar examples to give you more context when triaging the file.\n\n\n\nWhat do interpretable models look like?\nThere are two broad categories of ways to interpret models: transparency and post-hoc explanations.\n\nTransparency\nThis is basically “how does the model work?”. For example, simpler models such as linear regression are considered more interpretable than a deep neural network (which is sometimes referred to as a blackbox). Let’s imagine we want to train a model to predict house prices. We could have some features about the house such as the number of bedrooms, square footage, distance to a city with more than 500 000 people etc. A linear model is just taking these features and weights and adding them up.\n\\[ \\text{house price} = w_1 \\cdot (\\text{number of bedrooms}) + w_2\\cdot(\\text{square footage}) + w_3\\cdot(\\text{distance to city}) + ... \\]\nIn these models a higher weight means the feature is more important so interpreting which features are more important is really easy. To say the models are interpretable overall assumes that the features themselves are also easily interpretable (e.g. number of bedrooms).\nOther models may not be as easy to interpret directly. For example a random forest uses a bunch of decision trees (think flow charts). While the model may be harder to interpret overall, interpreting an individual decision tree is relatively straightforward.\nAnother reason that linear models are typically more interpretable than neural networks/deep learning is that we understand how they work in much more detail. For example, we can prove that a linear model will give a unique solution which is not the case with deep learning models. However, neural networks typically perform much better than other models. In order to get comparable performance out of these other models there may be tricks needed (e.g. feature engineering) which could make the model less interpretable overall.\n\n\nPost-hoc interpretability\nEven if we don’t directly know how the model works, we can still get useful information by interpreting the predictions. It’s worth noting that this is how humans explain decisions. We don’t always know the exact cause of a decision but can try to provide a rational explanation after the fact. Interpreting the model after the fact means that we can potentially use models with higher performance (e.g. deep learning models).\nOne method of post-hoc interpretability is to have an “explanation by example”. This means getting the model to predict which other examples the model thinks are most similar. In the case of predicting malware or not, the examples most similar to a malicious PDF may be other malicious PDF documents. In the case of deep learning models, we can also gain insight by visualizing the [hidden layers]((../2020-04-08-d-is-for-deep-learning/index.qmd).\nAnother technique that people use is to train two separate models. The first model makes the predictions, and the second model generates text that explains the prediction. For example, the first model may predict something about an image, while the second model generates a caption for that image.\nResearchers may also train two separate models where one model is more interpretable than the other. For example they may train a deep neural network (with high performance but low interpretability) as well as a random forest model. While the random forest model may not perform as well, it can also give some insight into the problem space. However, this could be potentially misleading as the explanations given by the simpler model may not correspond to why the more complex model works.\n\n\n\nThings to keep in mind\nInterpretability and having explainable models is still a highly active area of research. There are a few things to keep in mind when doing this in practice\n\nRules of thumb such as “linear models are more interpretable than neural nets” are generally true (but not always!)\nWhen talking about interpretability it is important to clarify what you mean exactly. I highly recommend reading Zachary Lipton’s paper for more specific definitions of the concepts above.\nThere may be cases where an ML model can perform significantly better than humans. Making sure the model is transparent (and potentially reducing performance) is not always the correct decision depending on the overall goal. It might be sufficient to have a better performing model with a human appeals process in place.\nI recommend using existing libraries such as interpret-ml which have a wide variety of methods and examples when trying to do this in practice.\n\n\n\nOther resources\n\nZachary Lipton’s paper\ninterpret-ml package\nPractical data ethics course from fast.ai\nPodcast on using Shapley values to interpret models"
  },
  {
    "objectID": "posts/2021-02-01-u-is-for-umap/index.html",
    "href": "posts/2021-02-01-u-is-for-umap/index.html",
    "title": "U is for UMAP",
    "section": "",
    "text": "Often in data science we have data with multiple dimensions/features that we want to visualize or embed for further analysis. UMAP (Uniform Manifold Approximation and Projection) is one method for doing dimension reduction which will help us with visualizations/embeddings. What do I mean by dimensions? You can think of a dimension as a column in a table or a spreadsheet. For example, let’s say we have a survey about people’s movie preferences. Each person was asked to rate a genre of movies on a scale of 0-5. By looking at the table, we can see that Alice and Mallory have similar tastes. Bob and Trent also seem to like the same genres as well. Since there are 5 different columns/movie genres we say that this data has 5 dimensions.\n\n\n\n\nComedy\nSci-Fi\nDrama\nHorror\nAction\n\n\n\n\nAlice\n5\n2\n4\n0\n2\n\n\nBob\n3\n3\n1\n5\n4\n\n\nMallory\n4\n2\n4\n1\n2\n\n\nTrent\n3\n3\n2\n5\n5\n\n\nWendy\n1\n5\n5\n3\n3\n\n\n\nTo get a better understanding of this data we want to visualize it. Unfortunately, unless you live in the Interstellar universe, humans can only see in 3 dimensions or less. We want to find a way to represent this data in 2 dimensions where points that are close in the higher dimensional space are close in the lower dimensional space. More concretely, in the new 2D representation, Alice should be close to Mallory and Bob should be close to Trent (because they are similar in the higher dimension). Dimension reduction is a general term for any method that represents high-dimensional data into a lower dimensional space. It’s also known as embedding your data into a lower dimensional space (and I’ll use the terms interchangeably). The two new dimensions of the embedding are related to the original features but are not just a subset. In our movie example it is not just as simple as saying “Comedy and Horror are the important features”. How you combine the original features into a new representation depends on the method of dimension reduction used.\n\nMethods for dimension reduction\nThere are many methods for doing dimension reduction but here are three popular ones:\n\nPrincipal Component Analysis (PCA). This is fast, well understood, and the resulting dimensions are kind of interpretable.\nt-distributed stochastic neighbor embedding (t-SNE). Mostly used for visualization and is used in a wide variety of applications.\nUniform Manifold Approximation and Projection (UMAP). Developed by Leland McInnes and John Healy (who also made HDBSCAN it has been growing in popularity in recent years. It’s faster than t-SNE and arguably preserves the higher dimensional structure better than t-SNE.\n\n\n\nUsing UMAP for document embedding/clustering\nI’m intending this blog to give you the overall flavour of how dimension reduction methods like UMAP are used. If you’re interested in using UMAP I recommend reading the wonderful getting started guide in the documentation. There’s a great video from Leland McInnes giving a higher level intuition of how it works. If you’re brave (or have a background in topological data analysis) you can dig into the mathematical details of why UMAP works.\nAs a more concrete example let’s imagine we want to find groups of related forum posts. We can use the 20 newsgroups dataset which is a collection of newsgroup documents across different topics. This example combines a lot of ideas we’ve seen in previous blog posts. If you want code and further explanations to go along with this example, I recommend reading this [document embedding tutorial][https://umap-learn.readthedocs.io/en/latest/document_embedding.html].\nWe want to find groups of related things, which sounds like we’ll need to use a clustering algorithm. The clustering algorithm can’t just use the raw data on its own, so we’ll need to pass in an embedding. You might recall from “E is for Embeddings” that an embedding requires two things:\n\nA numeric representation of your data (because we need to do math)\nA distance measure (so we can determine how close/far two points are from one another)\n\nLet’s start with the numeric representation aspect. Forum posts are mostly text so we’ll need to use natural language processing (NLP) techniques. To represent a forum post, we can just count how often a word from a vocabulary appears in that post. This is known as count vectorization and there are a lot more details in “N is for NLP”. This will give us a word-document matrix that could look something like\n\n\n\n\nthe\npizza\nbaseball\n…\nCPU\n\n\n\n\nPost 1\n7\n0\n2\n…\n0\n\n\nPost 2\n15\n0\n0\n…\n2\n\n\nPost 3\n10\n1\n3\n…\n0\n\n\n\nEach row corresponds to a forum post and each column represents a word in the vocabulary. And the value of each cell in the table is how often that word appeared in a given post. Now we have a numeric representation of our data! Now we just need a way of measuring distance between two posts. For example, it looks like post 1 and post 3 are similar because they are both talking about baseball. The fact that we have a series of counts makes Hellinger distance a good choice. There are more details on Hellinger distance in “J is for Jaccard Metric”.\nIf we calculate the word-document matrix for the 20 newsgroups dataset we get 34880 unique words over 18846 forum posts! This also ignores all words that occur less than 5 times in the whole corpus. Now as you can imagine this is a sparse matrix (there are a lot of zeroes) because most forum posts don’t have 35000 unique words in them. This means that all of the points are spread far apart. Density based clustering algorithms like [HDBSCAN](../2020-07-01-h-is-for-hdbscan/index.qmd assume that clusters are groups of points that are close together. We need to use dimension reduction to go from 34880 dimensions to a lower dimensional space.\nIn 34880 dimensions, all the points are spread far apart. However, while 2D is useful for visualization, the points can get smushed together and you can lose information which will help you distinguish groups of posts. Fortunately, UMAP lets you embed into an arbitrary number of dimensions such as 10, 25, or 50. I find it is useful to try clustering on data that has been embedded into 10 or 20 dimensions as you data is no longer sparse, but you still can keep a lot of information about the individual data points. Finding which lower dimensional space works best is mostly a matter of trying different values and seeing what works best. I recommend trying different values like 10, 20, 40 (as compared to 10, 11, 12 etc).\nWe can also embed the 20 newsgroups data into 2D for visualization and colour the points by the forum topic (e.g. rec.sport.hockey or comp.sys.ibm.pc.hardware). This was done by count vectorizing the data and using Hellinger distance as described.\n\nThis embedding looks pretty good. There are groups that you would expect to be together (e.g. the sports related topics). Additionally, the big clump in the middle has topics that are very related (computer hardware). On this embedding (or one in a slightly higher dimension) we can run a clustering algorithm to find all the related posts without having to use the topic labels.\n\n\nSummary\nUMAP is a great tool to have in your data science toolbox for dimension reduction. This blog barely scratches the surface of what it can do but here are some highlights\n\nUMAP works on a wide variety of data types such as images, text, and tabular data. As long as you have a numeric representation of that data and a meaningful distance measure you can embed it with UMAP.\nThe canonical implementation of UMAP is in Python but there are implementations in other languages such as R.\nUMAP is actively developed and there are useful features being added all the time. For example, the latest release makes it easier to align embeddings over time which is incredibly powerful.\nYou can also combine dimension reduction methods like PCA and UMAP. For very high dimensional data it is a common workflow to apply PCA first and then UMAP on the embedded data.\n\n\n\nOther resources\n\nEmbed all the things - John Healy (talk from Pydata Los Angeles 2019)\nUMAP documentation\nUMAP Uniform Manifold Approximation and Projection for Dimension Reduction - SciPy 2018\nAn episode of the podcast Linear Digressions on UMAP\nA One-Stop Shop for Principal Component Analysis"
  },
  {
    "objectID": "posts/2020-10-09-l-is-for-labelling-data/index.html",
    "href": "posts/2020-10-09-l-is-for-labelling-data/index.html",
    "title": "L is for Labelling Data",
    "section": "",
    "text": "Having high quality labelled data is critical to training useful supervised learning models. This labelled training data needs to be directly relevant to the real world task you are trying to accomplish. Unfortunately, getting this labelled data can be very challenging in a lot of cases. This is for a couple reasons:\n\nLabelling data is time intensive and also really boring/tedious. You often need hundreds or thousands of examples (or more!) so it takes a while to create a labelled training set. People are also incentivized/rewarded for training a useful ML model, as compared to labelling 1000s of points.\nHaving high quality labels often requires input from subject matter experts (whose time is valuable). For example, looking at x-ray images to label the images to have a given disease. This would require a medical professional who may not have the time required to label 1000s of images. Even if the data scientists themselves are doing the labelling, they are also typically highly paid professionals who may not have the time or commitment from management to spend hours labelling data.\n\n\nLabelling data in practice\nFortunately in the real world there are a few ways to make getting a labelled data set easier (not easy, but easier).\n\nPay someone else to do it\nThere are an increasing number of services where you can outsource your data labelling tasks. This includes Amazon’s SageMaker Ground Truth or Google’s AI Platform Data Labelling Service. These services can work if you have a large budget. However, if the dataset you are trying to label is sensitive (e.g. medical records) or requires a large amount of subject matter expertise then you may not be able to use these services. Alternatively, just pay grad students to label data do research.\n\n\nMake people do it for free\nYou’ve probably needed to go through an image based captcha (e.g. “select all the pictures with traffic lights”) in order to log into a popular web service. If you’re a large organization or have a product that many people use, this can be an effective way to get people to label data for free.\n\n\n\nUse pretrained models or datasets\nIn some cases there may be publicly available models (e.g. modelzoo) that have been trained on a similar task to the one you’re interested in. Similarly, there are publicly available labelled datasets (e.g. kaggle datasets) that you could train your own model on. These datasets/models could come from academic researchers, companies such as Google, or open source projects. If the pretrained model gives acceptable results for your problem then great! It is worth thinking about how your dataset/task is different than the one the model was trained on. If your data diverges from the original dataset over time, then your model will be less accurate. This is known as model drift. It’s possible to fine-tune these pretrained models onto your dataset/labels to improve their performance for your task. I’ll talk about that in more detail in T is for Transfer Learning.\n\n\nUsing unsupervised learning\nInstead of labelling each point individually, you can often try clustering your data beforehand. This allows you to label groups of data instead of each individual point.\n\nYou can build this clustering and labelling system yourself or use a labelling tool which does the same thing. Some of these include (most you need to pay for):\n\nplatform.ai for images\nprodigy for text\nSnorkel has a paid and a free product\n\n\n\nData augmentation\nOne of the reasons for having large training sets is to have many examples for the model to learn patterns. If you are trying to classify images of dogs you need to have many different pictures of different breeds. The pictures need to be from different angles (head on, side view, etc.) to be able to recognize the dogs in different scenarios. One way to increase the size of your dataset is to use data augmentation. If you have a picture you can generate new labelled data by transforming the original image. This might include blurring, cropping, or rotating the image. Using this technique you can get multiple labelled data points for each image that was labelled by a human. Having multiple versions of an image with varying amounts of noise will make your model more robust as well. Data augmentation can be applied to other data types as well (e.g. text) but it’s slightly trickier than doing it with images.\n\n\n\nActive learning\nThis is a technique where you can use a ML model to help you label your data. It works as follows:\n\nLabel a small amount of your training set (e.g. 10%)\nTrain a ML model on the labelled data you have\nUse that ML model to predict labels for the other 90%. You can verify the labels for the predictions where the model was not very confident. You should also randomly check predictions where the model was confident just to check that it is working as intended.\n\nUsing this method can help speed up the data labelling process and give you a better model overall.\n\n\n\nSummary\nGetting labelled data can be one of the most challenging parts of training a supervised learning model. If you aren’t able to throw money at the problem (most of us aren’t!) then there are many techniques you can try to increase your productivity when labelling data such as using labelling tools or data augmentation. When using existing datasets and models make sure to think about the similarities and differences between that dataset and the problem you want to tackle.\n\n\nOther resources\n\n7 Ways to Get High-Quality Labeled Training Data at Low Cost\nActive Learning - Computerphile"
  },
  {
    "objectID": "posts/2021-01-31-t-is-for-transfer-learning/index.html",
    "href": "posts/2021-01-31-t-is-for-transfer-learning/index.html",
    "title": "T is for Transfer Learning",
    "section": "",
    "text": "When you set out to learn something new, you typically aren’t starting completely from scratch. For example, you probably didn’t have to learn how to read English just to read this blog. Transfer learning is a way to take a model that has already been trained on a related problem and use it as a starting point. This has a few benefits:\n\nMany sophisticated models take a lot of time/money/computational power to train. Being able to start from these models reduces the overall cost of training a new model.\nTransfer learning typically improves performance (in addition to being easier to train).\nYou typically need less labelled data for your problem with transfer learning. As I’ve blogged about before getting labelled data can be challenging.\n\nTransfer learning is most commonly applied to [deep learning]../2020-04-08-d-is-for-deep-learning/index.qmd models for computer vision (images/videos) and natural language processing applications (text).\n\nClassifying cats and dogs\nOne of the most popular introductory Kaggle competitions is classifying images of cats and dogs. Here we need to predict if an image contains a cat or a dog.\n\nInstead of training the model completely from scratch, we can start with a model trained on the ImageNet dataset. This is a dataset with millions of images that all belong to thousands of categories. For example, some images are categorized as “flamingo” or “basketball”. The ImageNet challenge is a competition to train the best model to predict the correct class out of 1000 preselected classes. Many of these classes are specific dog breeds (e.g. “Pembroke, Pembroke Welsh corgi”) but we are interested in the more generic question of “dog or cat?”. We can take a model that has already been trained on the ImageNet dataset and fine-tune it to our specific problem. These pretrained models are typically deep neural networks (specifically convolutional neural networks or CNNs).\nAs a refresher, let’s look at how a neural network works at a high level. There is an input layer where the raw data is fed. In the case of images this is pixels. There is an output layer which has a neuron for each class. In our case we have two classes: cats and dogs. In the middle there are hidden layers with weights for each feature. In deep neural networks the features come from the previous layer (e.g. the first hidden layer uses the input layer, the second layer uses the first etc).\n\nWhen we train a model from scratch, these weights are initialized randomly. In transfer learning, we start with the weights from the pretrained model. This means that we need to use the same model architecture (number of hidden layers, how they are connected etc). Popular pre-trained models/architectures for this ImageNet include VGG16, VGG19, ResNet50, Inception V3, and Xception (though you don’t really need to worry about this). We can change the number of neurons in the output layer, as the original model had 1000 neurons in its output layer while we only have two.\nIn CNNs the earlier layers (closer to the input layer) typically learn to identify more basic features about the images. For example, the first layer could identify things like horizontal or vertical lines. The second layer could identify corners while later layers can identify much more complicated structures like eyes or paws. The relevant features (horizontal/vertical lines) from the earlier layers probably won’t change much between the pretrained model and our model. The last few layers are much more likely to be different between the pretrained model and our model. For example, features that were relevant to identifying a basketball are not particularly useful for the cats vs dogs problem. Of course, this isn’t true if all of your dog pictures come from the Air Bud movies. One technique that people use when fine-tuning models is layer freezing. This means that you don’t let the weights in the earlier layers change, while the weights in later layers can be updated. A related technique is called discriminative fine-tuning which is where you have different learning rates for each layer.\n\n\n\nTransfer learning for NLP\nTransfer learning has been applied to computer vision for years. In the past couple of years it has also gained a lot of traction in NLP. Sebastien Ruder, one of the authors of ULMFiT (Universal Language Model Fine-tuning for Text Classification), wrote a blog post titled “NLP’s ImageNet moment has arrived”. In this post he outlines three methods for doing full transfer learning on text, including ULMFiT, ELMo, and Transformers. Historically, the first layer of deep learning models for NLP were initialized with word embeddings. The rest of the network was initialized randomly and needed to be trained from scratch. Each of these techniques uses a different approach (though they all use pretrained language models), but I’ll talk about how ULMFiT works at a high level.\nLet’s imagine we want to train a classifier to predict if an email is spam/not spam. To give ourselves a headstart we can start with a language model. You might remember from N is for NLP that a language model is just a model that tries to predict the next word in a body of text. The corpus (set of documents) used to train the language model doesn’t need to be the same as what you are using for your downstream task (spam classification). A common approach is to train a language model on a large corpus such as English wikipedia. Training a model on all of wikipedia takes a huge amount of computational time and resources. Fortunately, someone has already done this and we can fine-tune our language model on the documents that we care about. When we are fine-tuning the language model, we start with a model that has been trained to predict the next word in a Wikipedia article. We then train the model to try to predict the next word in an email. Once we have fine-tuned our language model, we can slightly modify it to do text classification (spam/not spam).\n\nIn practice there are a few things to keep in mind:\n\nWhen using a pretrained model, you need to perform the exact same [preprocessing]../2020-10-10-m-is-for-munging-data/index.qmd that was done for the original model.\nThis method assumes that the way language is used in your corpus (e.g. emails) is similar to the general corpus (e.g. Wikipedia). It might not work as well if you try to predict tweet sentiments using academic papers as the general corpus.\nOne benefit of this approach is that you don’t need extra labelled data for fine-tuning the language model. Let’s say you have 1500 emails labelled as spam/not spam. However, you might have an extra 50 000 emails without labels. Because the language models are self supervised (the word you are trying to predict is the label), you can use this extra unlabelled data to improve the performance of your model.\n\n\n\nSummary\nTransfer learning is an extremely powerful technique that can improve performance, training time, and reduce the amount of labelled data required. It’s worth keeping in mind that you will inherit any bias that exists in the pretrained models (or their underlying datasets). You also need to make sure the dataset used to train the pretrained model is similar to the dataset you are trying to use. Starting with a model trained on images of puppies will not be helpful when trying to classify medical images. Knowing when to use pretrained models comes with experience as well as asking domain experts.\n\n\nOther resources\n\nTransfer learning (fastai NLP video 9)\nNLP’s ImageNet moment has arrived\nImageNet\nA Gentle Introduction to Transfer Learning for Deep Learning"
  },
  {
    "objectID": "posts/2020-10-25-p-is-for-pandas/index.html",
    "href": "posts/2020-10-25-p-is-for-pandas/index.html",
    "title": "P is for Pandas",
    "section": "",
    "text": "Pandas is a very popular data analysis library for python. It’s an invaluable tool for transforming and munging data. Some of my favourite features include:\n\nIt can read data from a ton of formats including csv, json, and database tables\nThere are lots of convenience features built in such as easy plotting, filling in missing data, dropping duplicates, filtering data etc\nIt’s compatible with a lot of other python data science libraries (e.g. numpy, scikit-learn, etc)\n\nIt makes it really easy to read and explore your data since you can read, filter, and plot your data in just a few lines of code. This blog is not going to be a comprehensive tutorial (there are actual tutorials linked in the Other Resources section below). It’s intended to give you a brief idea of the kinds of things that pandas can do (to save you from rewriting code to do that yourself). The documentation is quite good and will give you an idea of the types of analysis you should use pandas for (hint: most of them).\nThe main data types in pandas are series and dataframes. You can think of a dataframe as a spreadsheet or a table in a database. Dataframes are made up of series (just think of a series as a column in a spreadsheet/table).\n\nUsing pandas\n\nInstalling and importing pandas\nIf you’re already familiar with installing python packages use conda install pandas or pip install pandas. For more in depth instructions there is a guide here. Once you have it installed you can import it using\nimport pandas as pd\nPeople conventionally rename pandas to pd (e.g. pd.DataFrame instead of pandas.DataFrame) since you will end up typing pandas/pd a lot.\nHere I’m going to use the movie data used in Brandon Rhode’s “Pandas From the Ground Up” tutorial. This includes CSV (comma separated value) files with movie titles and release dates as well as casting. After we’ve downloaded the data we want to load it into a dataframe. We can do that using\ntitles = pd.read_csv('titles.csv')\nIf we want to look at the first 10 rows of the dataframe we can use the head method\nprint(titles.head(10))\nwhich will output the first N rows (in this case 10). This is an easy way to see what the data looks like and if the data is well formatted.\n\n\n\n\ntitle\nyear\n\n\n\n\n0\nLusty Neighbors\n1970\n\n\n1\nThe Adventures of Priscilla, Queen of the Desert\n1994\n\n\n2\nCaptain Sindbad\n1963\n\n\n3\nCold Dark Mirror\n2015\n\n\n4\nOstatnia Rawa Ryska Riedla\n1997\n\n\n5\nForever and Ever\n2018\n\n\n6\nWestern Conviction\n2018\n\n\n7\n5 Estrellas\n2018\n\n\n8\nRopewalk\n2000\n\n\n9\nSally in Our Alley\n1931\n\n\n\n\n\nSelecting and filtering data\nWe can easily select rows in our dataframe. For example if we wanted to find movies released in 1991\ntitles[titles['year'] == 1991]\n\n\n\n\ntitle\nyear\n\n\n\n\n71\nLe voleur d’enfants\n1991\n\n\n132\nMadreseye piremardha\n1991\n\n\n196\nPyat pokhishchennykh monakhov\n1991\n\n\n217\nCanh bac\n1991\n\n\n509\nItakwil man ako ng langit\n1991\n\n\n\nLet’s break down the above statement a bit. We use titles[‘year’] to select the year column in the data frame. titles[‘year’] == 1991 returns a series of booleans (True if the year is 1991 and False otherwise). Finally, titles[titles['year'] == 1991] says “give me the rows from titles where the condition is True”. We don’t just have to look for rows with exact matches. Let’s search for movies containing “The Hobbit” in the title\ntitles[titles['title'].str.contains('The Hobbit')]\n\n\n\n\ntitle\nyear\n\n\n\n\n121727\nThe Hobbit: The Battle of the Five Armies\n2014\n\n\n146926\nThe Hobbit: An Unexpected Journey\n2012\n\n\n166752\nThe Hobbit: The Desolation of Smaug\n2013\n\n\n179646\nThe Hobbit: The Swedolation of Smaug\n2014\n\n\n\n\n\nCounting values\nLet’s say we want to count how many movies were released per year. We can do this using\ntitles['year'].value_counts()\nwhich gives a sorted list from highest count to lowest. In our data set we see that 2017 released the most movies followed by 2016. I suspect that this dataset was compiled in 2017 and that 2018 had more movies released in it than the previous year.\n\n\n\nyear\ncount\n\n\n\n\n2017\n9888\n\n\n2016\n8198\n\n\n2015\n7564\n\n\n2014\n7159\n\n\n2013\n6896\n\n\n\nThe value_counts function also has a normalize parameter which divides the counts by the total number of rows.\ntitles['year'].value_counts(normalize=True)\n\n\n\nyear\nfrequency\n\n\n\n\n2017\n0.0437497\n\n\n2016\n0.0362722\n\n\n2015\n0.0334671\n\n\n2014\n0.0316752\n\n\n2013\n0.0305115\n\n\n\nHere we can see that about 4% of all movies ever released were released in 2017.\n\n\nPlotting data\nIf we want to plot the number of movies released per year over time how would we do that? In pandas we can just do\ncounts_per_year = titles['year'].value_counts()\n# This just sorts it so that the years will be in order\ncounts_per_year = counts_per_year.sort_index()\ncounts_per_year.plot()\n\nIt looks like the number of movies being released has been increasing over time. There are also movies in the dataset that are scheduled for release (including one in 2115!) which explains the sharp dropoff. The .plot() methods that wrap matplotlib so you can customize your plots as much as you want.\n\n\nGrouping and merging data together\nA common (but slightly more advanced) use case for pandas is grouping data together. Imagine we want to see which actors/actresses mostly played leading roles in their career (who appeared in at least 50 movies). To do so we will first load in the cast dataset\ncast = pd.read_csv('cast.csv')\nprint(cast.head())\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\nyear\nname\ntype\ncharacter\nn\n\n\n\n\n0\nCloset Monster\n2015\nBuffy #1\nactor\nBuffy 4\n31\n\n\n1\nSuuri illusioni\n1985\nHomo $\nactor\nGuests\n22\n\n\n2\nBattle of the Sexes\n2017\n$hutter\nactor\nBobby Riggs Fan\n10\n\n\n3\nSecret in Their Eyes\n2015\n$hutter\nactor\n2002 Dodger Fan\nnan\n\n\n4\nSteve Jobs\n2015\n$hutter\nactor\n1988 Opera House Patron\nnan\n\n\n\nHere we have the title of the movie, the year it was released, the name of the actor/actress, if they were an actor/actress, the character name, and the listing in the credits (n). A listing number of 1 means they played a leading role and higher numbers usually correspond to extras. Some of the values of n are “nan”, which stands for “not a number”. We want to drop those rows to only get listed roles.\nlisted_roles = cast.dropna(subset=['n'])\nNow we want to find how many movies easy actor appeared in\nnum_roles = cast.groupby('name').size()\n\n\n\nname\n\n\n\n\n\nJames Millican\n67\n\n\nEi Kimura\n1\n\n\nBrian Torpe\n1\n\n\nSteve Edis\n1\n\n\nCarlos Esteban Fonseca\n3\n\n\n\nNote that this is roughly equivalent to the value_counts method. Now we want to select the actors who appeared in at least 50 movies\nin_lots_of_roles = num_roles[num_roles&gt;50].reset_index(name='total_roles')\n\n\n\n\nname\ntotal_roles\n\n\n\n\n0\nA. Bromley Davenport\n52\n\n\n1\nA.K. Hangal\n87\n\n\n2\nAbdur Razzak\n63\n\n\n3\nAbhi Bhattacharya\n63\n\n\n4\nAbhishek Bachchan\n52\n\n\n\nNow we need to get all the roles each of those actors played. We can do this by merging this dataset with the listed_roles dataframe.\nmerged = pd.merge(in_lots_of_roles, listed_roles)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\ntotal_roles\ntitle\nyear\ntype\ncharacter\nn\n\n\n\n\n0\nA. Bromley Davenport\n52\nA Maid of the Silver Sea\n1922\nactor\nOld Tom Hamon\n3\n\n\n1\nA. Bromley Davenport\n52\nA Sister to Assist ’Er\n1927\nactor\nJim Harris\n4\n\n\n2\nA. Bromley Davenport\n52\nBonnie Prince Charlie\n1923\nactor\nSir John Cope\n11\n\n\n3\nA. Bromley Davenport\n52\nBoy Woodburn\n1922\nactor\nMatt Woodburn\n3\n\n\n4\nA. Bromley Davenport\n52\nCaptivation\n1931\nactor\nColonel Jordan\n6\n\n\n\nFinally, we can see which actors played mostly leading roles in their career. We will group by the actors name, then take the average of their listing number. The closer to 1.0 the number is the more leading roles they played. We will sort the values to see the actors closest to 1.0 at the top.\nmerged.groupby('name')['n'].mean().sort_values()\n\n\n\nname\nn\n\n\n\n\nWilliam S. Hart\n1\n\n\nYilmaz Güney\n1.01724\n\n\nMary Pickford\n1.03509\n\n\nTom Mix\n1.06186\n\n\nMary Miles Minter\n1.09804\n\n\n\nIt appears that William S. Hart had a leading role in every film that he appeared in.\n\n\n\nSummary\nPandas is an extremely powerful python library for doing data analysis. There can be a bit of a learning curve but it is worth spending some time exploring the library. Once you can perform some basic tasks (sorting, filtering, groupbys, plotting) then you will become a data analysis wizard!\n\n\nOther resources\n\nPandas tutorial series from Corey Schafer\nPandas cookbook\nPandas From the Ground Up"
  },
  {
    "objectID": "posts/2020-10-23-o-is-for-outlier-detection/index.html",
    "href": "posts/2020-10-23-o-is-for-outlier-detection/index.html",
    "title": "O is for Outlier Detection",
    "section": "",
    "text": "Outlier (or anomaly) detection is the technical term for “finding weird stuff”. It’s used in a wide variety of applications including malware detection and looking for credit card fraud. For example, if you live in Ottawa but your credit card was used to buy a gaming console in Boise, Idaho (without any other purchases) that would be anomalous. Outlier detection is related to clustering. In clustering we are trying to find the groups of related data. In outlier detection we are trying to find the points that don’t belong to any groups. There are three different categories of outliers\n\nLocal (or contextual) outliers: These are points that are close to groups of data but don’t belong to any cluster. This could be an email which seems mostly legitimate except something seems a little off (e.g. “Follow us on Twittterr”) .\nGlobal (or point) outliers: These are the data points that are completely off on their own and are far away from other data points. Going back to the email example, this would be like having an email in German if the rest of your inbox had English/French emails.\nCollective outliers : These are groups of outlying points which may have some underlying pattern. This would be like having a spam campaign of emails where they look strange compared to normal email, but are all related to one another.\n\n\nIn many (all?) large datasets there are bound to be lots of outliers. It’s like being a woman trying to find a partner on a dating app: “the odds are good but the goods are odd”. It’s relatively easy to find anomalies but it’s more challenging to find interesting outliers.\n\nIt’s worth noting that in a lot of cases if something is anomalous it doesn’t necessarily mean that it is bad. It just means that the data point is different from the others. However, it probably means that it is more interesting and may need to be investigated.\nTo find out if an outlier is truly interesting or not you need to combine it with extra context. For example, if you find an anomalous credit card purchase (such as a gaming console in Idaho) it’s worth looking at the other purchases around that time (did they also buy plane tickets to Idaho?).\nYou can also find interesting patterns by grouping anomalies together. In malware detection this could mean “did a bunch of suspicious activity happen on one computer in a short time frame?”. Here you need something else to group on (in this case it’s by looking at anomalies for one computer at a time).\n\n\nHow do we actually find anomalies?\n\nLow frequency events\nOne of the easiest ways to find outliers is to say “show me events that occur less than X% of the time”. This is really easy because we can just count all the events we are interested in and then divide by the total number of events. Let’s imagine we have a survey of what people put in their coffee\n\n\n\n\nRaw count\nFrequency (count/total)\n\n\n\n\nNothing (black)\n95\n0.35\n\n\nMilk\n120\n0.44\n\n\nSugar\n53\n0.20\n\n\nButter\n2\n0.01\n\n\n\nIn this case it’s pretty clear that people putting butter in their coffee is weird. This method breaks down if there are lots of different options or one or two very popular options.\n\n\nModelling the distribution of your data\nOne way to find outliers is to look at how far a point deviates from the average (mean). If your data is normally distributed (i.e. looks like a bell curve) you can look at how many standard deviations a point is from the mean. In a normal distribution 68% of the data falls within one standard deviation, 95% within two standard deviations etc. For example, if the average height of a man is 5’10” and the standard deviation is 4” then 68% of men are between 5’6” and 6’2”. If you have a man who is 7 feet tall, then they would be more than 3 standard deviations away from the mean. It’s fair to say that this person is abnormally tall (unless you compare them only to NBA players).\n\n\n\nTaken from https://upload.wikimedia.org/wikipedia/commons/a/a9/Empirical_Rule.PNG\n\n\n\n\nUsing clustering to find outliers\nSome clustering algorithms (such as HDBSCAN label points which don’t fall into any cluster as outliers. These points could be global or local outliers. They may also include the probability that a point belongs in a cluster. This can be used to find local outliers by looking for points with relatively low probabilities. If we look back to the sample dataset in my post on HDBSCAN, all of the grey points are listed as noise/outliers.\n\n\n\nTime series modelling\nIn many cases what counts as an anomaly depends on when it occurs. For example, a store doing $100 000 in sales in one day could be abnormally high in February but unusually low in the holiday season. The average amount of sales changes throughout the year so we need to take time into account when doing outlier detection. Prophet is a popular python library for dealing with time series data.\n\n\nIsolation forests\nOne way we can find anomalies is using a method called isolation forests. Isolation forests take advantage of two key properties of anomalies:\n\nThere are fewer of them\nThey have attributes that are different than most of the points\n\nThese properties mean that the anomalies are more prone to isolation. What do I mean by isolation? Imagine I have the data points shown in blue below.\n\nThe idea behind isolated forests is to keep making cuts in the data so that each point is in its own partition. From these cuts we can build up trees to use in our forest. For example:\n\nIn this case, the first cut is shown in red. Everything above it is in one partition (which in this case is an isolated point), while everything below it is in another. We can make a cut to the bottom partition (shown in black) and isolate another point. We can keep cutting and build up the tree. At the end, every leaf node will correspond to an isolated point. The hypothesis is that anomalies will be closer to the root node than “normal” points. The algorithm works as follows:\n\nTake a random subset of the data and build up the tree for that subset.\nRepeat step 1 a bunch of times. Since all of the cuts are fairly arbitrary, we will use an ensemble of trees to average out our choice of cuts.\nFrom the forest of trees calculate the average path length to each leaf node. This just means “on average how many cuts did we need to make to isolate that point”.\nUse the average path length to calculate a score which we can use to determine if something is an outlier. The smaller the average path length (i.e. we needed less cuts), the more likely a point is to be an outlier.\n\nThis algorithm is useful for a few reasons:\n\nTypically outlier detection algorithms work by profiling what is “normal” and then finding deviations from that. This isn’t required when using isolation forests\nIt’s fast (and scales fairly well)\nIt doesn’t require a distance metric\nThere is an implementation of it in the popular python ML library scikit-learn\n\n\n\n\nSummary\nAnomaly detection is a task that shows up in many different applications. It’s typically used when “we want to find something interesting but we don’t know what that is”. In order to find the truly interesting data points, you need to add context by combining them with other information. This is especially important if you want to take an action (e.g. cancelling someone’s credit card) based on anomalous activity.\n\n\nOther resources\n\nA brief overview of outlier detection techniques\nIntroduction to anomaly detection in python"
  },
  {
    "objectID": "posts/2020-09-27-j-is-for-jaccard/index.html",
    "href": "posts/2020-09-27-j-is-for-jaccard/index.html",
    "title": "J is for Jaccard metric",
    "section": "",
    "text": "In previous blogs we’ve talked about choosing a distance measure as a way of saying “these two things are close if …”. One useful measure is Jaccard similarity/distance since it measures the similarity between two sets. This is useful if you have a lot of categorical variables (i.e. ones that don’t have any inherent ordering). For example, two people are probably similar if they have the same sets of interests/hobbies. The Jaccard similarity of two sets is just the size of the intersection divided by the size of the union. Or put visually:\n\n\nJaccard similarity/distance\nAs a more concrete example let’s imagine that we have collected a list of people’s favourite pizza toppings and we want to find which people are most similar.\n\nKara likes pepperoni, mushrooms, and green pepper\nZach likes ham, pineapple, and jalapeno peppers\nRodney also likes pepperoni, mushrooms, and green peppers\nSophie likes olives, ham, pepperoni, and mushrooms\n\nSo what are the Jaccard similarities between these people?\n\nKara and Rodney like exactly the same toppings so their similarity is 1\nRodney and Zach have nothing in common so their similarity is 0\nKara and Sophie have some things in common but Sophie enjoys more toppings. Their similarity is 0.4 ([pepperoni, mushrooms]/[pepperoni, mushrooms, olives, ham])\n\nYou’ll notice that two sets that are exactly the same have a similarity of 1. To change this into a distance we just to\n\\[ D_{Jaccard} = 1 - similarity \\]\nNow Kara and Rodney have a distance of 0, while Rodney and Zach are a distance of 1 (which in this case is as far apart as you can be).\n\n\nHellinger distance\nYou might have noticed that Jaccard distance doesn’t take into account how frequently the items in the set occur. If counts do matter for your problem, then you will want to use Hellinger distance. Let’s imagine you have some data on the number of times people have read a given book:\n\n\n\n\n\n\n\n\n\n\n\n\nHarry Potter\nHello World\nThe Hobbit\nThe Great Gatsby\nTrick Mirror\n\n\n\n\nMarie\n5\n0\n1\n1\n0\n\n\nJordan\n1\n0\n1\n1\n1\n\n\nSarah\n0\n2\n0\n1\n1\n\n\nPatrick\n4\n0\n2\n1\n0\n\n\n\nMarie, Jordan, and Patrick have all read Harry Potter which would make them similar under Jaccard similarity. However, Marie and Patrick are probably more similar since they both read it multiple times. Hellinger distance takes this into account. I’ll try to give you some intuition for how it does this.\nImagine that each set of counts is generated by some weighted multi-sided die (that is different for each person). When we roll Maries die, it is more likely to come up with Harry Potter and less likely to come up with Hello World. The opposite is true for Sarah’s die, which is more likely to come up with Hello World and less likely to come up with Harry Potter. We calculate what the weights of these dice look like (these are called multinomial distributions).\nWe can then measure the mutual likelihood of these distributions. This just means “what is the probability of Jordans counts occuring using Maries die (and vice versa)”. If there is a high probability that Jordans counts occurred using Maries die, then Marie and Jordan should be considered close. If it is unlikely that Sarah’s counts occurred using Maries die, then Marie and Sarah should be pushed apart.\nHellinger distance is particularly useful if you have a bunch of text. You can consider two documents similar if you have the same words occurring at similar frequencies.\n\n\nSummary\nJaccard and Hellinger are both very useful distance measures that can be used in dimension reduction and embeddings. If counts matter, use Hellinger, otherwise use Jaccard distance.\n\n\nOther resources\n\nEmbed all the things - John Healy (talk from Pydata Los Angeles 2019)"
  },
  {
    "objectID": "posts/2020-02-01-b-is-for-bias/index.html",
    "href": "posts/2020-02-01-b-is-for-bias/index.html",
    "title": "B is for Bias",
    "section": "",
    "text": "The term bias is used in a few different contexts within data science. When people mention bias they are typically referring to either the “bias-variance tradeoff” or “unjust bias”. I’ll primarily talk about unjust bias in this blog. If you want to learn about the other kind of bias see “K is for K-fold cross-validation”.\n\nBias-variance tradeoff\nThe bias-variance tradeoff refers to tuning models such that they don’t under/overfit your data. A model has high bias if it underfits the training set (e.g. if you are trying to fit a line to non-linear data). A model has high variance if it overfits the training set (i.e. doesn’t generalize well). This could occur if you have a huge non-linear function with a large number of parameters. The bias-variance tradeoff is trying to find a model that minimizes both of these phenomena.\n\n\nUnjust bias and the importance of having a human in the loop\nMachine learning models are useful for helping humans sort through large piles of data. However, it is critical to have a human in the loop in order to validate predictions. It’s particularly important when ML models are used to predict things like university admissions or welfare benefits which have a huge impact on people’s lives. As we talked about in the previous blog, there are two key points to always keep in mind about AI/ML\n\nIt’s not magic\nIt’s not perfect\n\nProblems arise when people treat the output of ML models as completely objective (or magic) with no opportunities to overrule these predictions.\nBias refers to a model that is prejudiced in favour of particular categories. Unjust bias occurs when there is a mismatch between the models view of the world and how we think the world should be. There are many reasons that this can happen but two of the big ones are:\n\nThe training data is not truly representative. It may favour certain categories over others leading to better predictions on those categories. As the saying goes: “garbage in, garbage out”.\nThe training data is truly representative of past behaviour. However, this might be different than what we want the future behaviour to be.\n\nThere are a couple of famous examples of biased models which demonstrate this behaviour.\n\n\nFacial recognition\nFacial recognition tools are increasingly being utilized by organizations such as law enforcement agencies. Model fairness is critical in this case since if a model is biased against particular subgroups it will have a disproportionate impact on the lives of people in that subgroup. Researchers studied 3 commercially available facial recognition tools from Microsoft, IBM, and FACE++ (study is here). The researchers found that the models performed much better on men and also had higher accuracy on lighter skinned people. In the worst case there was a 34.4% difference in accuracy between lighter skinned men compared to darker skinned women. This shows when the training data does not accurately represent all subgroups the result is a biased model.\n\n\n\nTaken from http://gendershages.org\n\n\n\n\nAmazon’s hiring model\nLike many companies, Amazon receives huge numbers of applicants and sorting through these applications is incredibly time intensive. They had a historical set of applications and know which of those candidates were hired. So, they developed a machine learning model to sort through the applications and rank the candidates in terms of their likelihood to be hired. However, this essentially turned into a gender detection model since the vast majority of their previous hires were men. For example, the model would penalize resumes which included the word “women’s”. This is a case where a model can make accurate predictions based on historical data, however this does not match with how hiring should be done. Despite the fact that Amazon scrapped this model, they are far from the only company who would like to automate portions of their hiring process. Creating a fair and unbiased method of doing so that is interpretable is still an open research question. You should be suspicious of anyone who claims to have solved this problem.\n\n\n\nHow can we fix this?\nIt is important to note that humans are also biased. They can grade using different criteria if they get grumpy or tired. The appeal of using ML models is that they are much cheaper than humans and can scale much better. If left unchecked, this just means that biased decisions are made at a much larger scale than what was done previously. The best approach is to use a mixture of ML models while keeping a human in the loop. This means providing a way for people to appeal decisions made by an ML model (such as university/college admissions) and allowing a human to override the decision. As researchers, something to keep in mind is that the users of these algorithms may not understand probabilities/confidence intervals. Even if they do understand these concepts they may not feel comfortable overruling the ML model.\nCreating models that are fair and interpretable is still an active area of research. This is an incredibly complex and nuanced topic but it is important to be aware of it. Later on, I will write a blog about the different techniques you can use to interpret ML models to try and gain insight into how they are making decisions.\n\n\nOther resources\n\nGetting Specific About Algorithmic Bias) - Rachel Thomas\nWeapons of Math Destruction - Cathy O’Neil\nHello World - Hannah Fry"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "ABCs of data science is intended for anyone who wants to learn more about data science, regardless of skill level. It aims to give readers a high level overview of various data science concepts, so that they can explore these topics further. Note that these blogs were written before the explosion of LLMs but should hopefully provide some intuition into other data science techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA is for Artificial Intelligence\n\n\n\ndata_science\n\nAI\n\n\n\n\n\n\n\n\nJan 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nB is for Bias\n\n\n\ndata_science\n\nbias\n\ninterpretability\n\n\n\n\n\n\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nC is for Clustering\n\n\n\ndata_science\n\nclustering\n\nunsupervised_learning\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nD is for Deep Learning\n\n\n\ndata_science\n\ndeep_learning\n\nsupervised_learning\n\nAI\n\n\n\n\n\n\n\n\nApr 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nE is for Embeddings\n\n\n\ndata_science\n\nembeddings\n\nunsupervised_learning\n\nAI\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nF is for F1 score\n\n\n\ndata_science\n\nmetrics\n\nsupervised_learning\n\nAI\n\n\n\n\n\n\n\n\nMay 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nG is for Gradient Descent\n\n\n\ndata_science\n\noptimization\n\nsupervised_learning\n\nAI\n\n\n\n\n\n\n\n\nMay 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nH is for HDBSCAN\n\n\n\ndata_science\n\nclustering\n\nunsupervised_learning\n\n\n\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nI is for Interpretability\n\n\n\ndata_science\n\nsupervised_learning\n\nAI\n\nbias\n\n\n\n\n\n\n\n\nSep 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nJ is for Jaccard metric\n\n\n\ndata_science\n\nembeddings\n\ndistance_measures\n\n\n\n\n\n\n\n\nSep 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nK is for K-fold cross-validation\n\n\n\ndata_science\n\nsupervised_learning\n\nAI\n\n\n\n\n\n\n\n\nOct 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nL is for Labelling Data\n\n\n\ndata_science\n\nsupervised_learning\n\nAI\n\n\n\n\n\n\n\n\nOct 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nM is for Munging Data\n\n\n\ndata_science\n\ndata_cleaning\n\n\n\n\n\n\n\n\nOct 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nN is for Natural Language Processing (NLP)\n\n\n\ndata_science\n\nnlp\n\ntext_processing\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nO is for Outlier Detection\n\n\n\ndata_science\n\nunsupervised_learning\n\nanomaly_detection\n\n\n\n\n\n\n\n\nOct 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nP is for Pandas\n\n\n\ndata_science\n\ndata_cleaning\n\ndata_exploration\n\n\n\n\n\n\n\n\nOct 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nQ is for Q-learning\n\n\n\ndata_science\n\nreinforcement_learning\n\n\n\n\n\n\n\n\nDec 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nR is for Reproducibility\n\n\n\ndata_science\n\nreproducibility\n\n\n\n\n\n\n\n\nDec 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nS is for Supervised Learning\n\n\n\ndata_science\n\nsupervised_learning\n\nrandom_forest\n\ndeep_learning\n\n\n\n\n\n\n\n\nJan 17, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nT is for Transfer Learning\n\n\n\ndata_science\n\nsupervised_learning\n\npretrained_models\n\n\n\n\n\n\n\n\nJan 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nU is for UMAP\n\n\n\ndata_science\n\nembedding\n\ndimension_reduction\n\n\n\n\n\n\n\n\nFeb 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nV is for Visualization\n\n\n\ndata_science\n\nvisualization\n\nplotting\n\n\n\n\n\n\n\n\nFeb 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nW is for Wasserstein GANs\n\n\n\ndata_science\n\nsynthetic_media\n\ngans\n\ndeepfakes\n\n\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nY is for You Should Talk to Your Clients\n\n\n\ndata_science\n\n\n\n\n\n\n\n\nFeb 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nX is for XGBoost\n\n\n\ndata_science\n\nsupervised_learning\n\nxgboost\n\nensembles\n\n\n\n\n\n\n\n\nFeb 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nZ is for Zero to Done\n\n\n\ndata_science\n\n\n\n\n\n\n\n\nFeb 6, 2021\n\n\n\n\n\nNo matching items"
  }
]